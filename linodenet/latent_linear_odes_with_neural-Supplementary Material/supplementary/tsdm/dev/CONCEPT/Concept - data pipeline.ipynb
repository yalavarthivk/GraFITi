{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline Concept\n",
    "\n",
    "- we want to use pytorch DataLoaders for efficiency (prefatches batches in the background)\n",
    "- `DataLoader` requires `Dataset` + `Sampler` + `collate_fn`\n",
    "    - `Sampler`: produces `index` objects when iterated over.\n",
    "    - `Dataset`: values are looked up via `Dataset[index]`\n",
    "    - `collate_fn`: receives `[Dataset[next(iSampler)] for _ in batch_size]` and postprocesses it as desired.\n",
    "    \n",
    "Model requires **preprocessed** data, such as variable encoding, standardization, etc.\n",
    "The `preprocessor` must be fit on the training dataset. The whole pipeline looks something like this:\n",
    "\n",
    "\n",
    "\n",
    "- The target loss might be defined only for the original data, not the encoded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pipeline sketch - encode after sampling\n",
    "\n",
    "```python\n",
    "preprocessor.fit(train_data)\n",
    "\n",
    "# Construct pipeline objects\n",
    "dataset = make_dataset(train_data)  # original data space\n",
    "sampler = make_sampler(train_data)  # original data space\n",
    "collate = make_batcher(train_data)  # original data space\n",
    "dloader = DataLoader( ... )         # original data space\n",
    "\n",
    "# post-process the batch\n",
    "batch = next(iter(dloader))         # original data space\n",
    "inputs, target = get_items(batch)   # original data space\n",
    "\n",
    "# get model outputs\n",
    "model_inputs = preprocessor.encode(inputs)  # model data space\n",
    "model_target = preprocessor.encode(target)  # model data space\n",
    "model_output = model(model_inputs)          # model data space\n",
    "loss = loss_fn(model_output, model_target)  # model data space\n",
    "\n",
    "# evaluation\n",
    "predictions = decode(model_outputs)         # original data space\n",
    "eval_loss = score_fn(targets, predictions)  # original data space\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline sketch - encode before sampling\n",
    "\n",
    "```python\n",
    "preprocessor.fit(train_data)\n",
    "encoded_ds = preprocessor.encode(train_data)\n",
    "\n",
    "# Construct pipeline objects\n",
    "dataset = make_dataset(encoded_ds)  # model data space\n",
    "sampler = make_sampler(encoded_ds)  # model data space\n",
    "collate = make_batcher(encoded_ds)  # model data space\n",
    "dloader = DataLoader( ... )         # model data space\n",
    "\n",
    "# post-process the batch\n",
    "batch = next(iter(dloader))         # model data space\n",
    "inputs, target = get_items(batch)   # model data space\n",
    "\n",
    "# get model outputs\n",
    "model_inputs = preprocessor.encode(inputs)  # model data space\n",
    "model_target = preprocessor.encode(target)  # model data space\n",
    "model_output = model(model_inputs)          # model data space\n",
    "loss = loss_fn(model_output, model_target)  # model data space\n",
    "\n",
    "# evaluation\n",
    "predictions = decode(model_outputs)         # original data space\n",
    "eval_loss = score_fn(targets, predictions)  # original data space\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline sketch - both pre + post-processing\n",
    "\n",
    "```python\n",
    "preprocessor.fit(train_data)\n",
    "encoded_ds = preprocessor.encode(train_data)\n",
    "\n",
    "# Construct pipeline objects\n",
    "dataset = make_dataset(encoded_ds)  # model data space\n",
    "sampler = make_sampler(encoded_ds)  # model data space\n",
    "collate = make_batcher(encoded_ds)  # model data space\n",
    "dloader = DataLoader( ... )         # model data space\n",
    "\n",
    "# post-process the batch\n",
    "batch = next(iter(dloader))         # model data space\n",
    "inputs, target = get_items(batch)   # model data space\n",
    "\n",
    "# get model outputs\n",
    "model_inputs = preprocessor.encode(inputs)  # model data space\n",
    "model_target = preprocessor.encode(target)  # model data space\n",
    "model_output = model(model_inputs)          # model data space\n",
    "loss = loss_fn(model_output, model_target)  # model data space\n",
    "\n",
    "# evaluation\n",
    "predictions = decode(model_outputs)         # original data space\n",
    "eval_loss = score_fn(targets, predictions)  # original data space\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "1. Since inputs and targets may have different modalities, we may need multiple encoders, or, at the very least, a way to split of an sub-encoder responsible for treating the target data only.\n",
    "\n",
    "2. Alternatively, the encoding could happen \\*before\\* the data is \n",
    "\n",
    "3. Encode before sampling might be desirable for performance reasons, but impossible for memory limitations. For example, the full dataset might not fit into GPU memory, or even into RAM. Then, post sampling encoding becomes mandatory (or preprocess the whole dataset and store preprocessed version on disk alongside original data.)\n",
    "\n",
    "4. The preprocessor might do more than asked for, maybe easier to split of responsibilities?\n",
    "\n",
    "5. What is the \\*exact\\* relationship between the preprocessor and sampler / dataset objects?\n",
    "\n",
    "6. The get_items should possibly be included in the `collate_fn`\n",
    "\n",
    "7. The batch should possibly return more than the inputs and targets, possibly:\n",
    "\n",
    "- index that was used to look up the data\n",
    "  - This should probably be the responsibility of the dataloader, but anyhow.\n",
    "- original data (not just model inputs) for plotting purposes.\n",
    "- extra data that is not used by every model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea: TupleEncoder / MappingEncoder\n",
    "\n",
    "Add `TupleEncoder` representing product type, i.e. given `encoder = TupleEncoder(enc1, enc2, enc3)`, then\n",
    "\n",
    "```python\n",
    "encoder.encode( (a, b, c) )\n",
    "```\n",
    "\n",
    "is equivalent to\n",
    "\n",
    "```python\n",
    "(enc1.encode(a), enc2.encode(b), enc3.encode(c))\n",
    "```\n",
    "\n",
    "Likewise, we can have a `MappingEncoder` that acts as `encoder.encode({key:value})` being equivalent to `{key:encoder[key].encode(value)}` or `(encoder[key].encode(value))`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea: Allow Slicing or encoders\n",
    "\n",
    "For example: Allow slicing of `Standardizer`. Since this encoder works element-wise, it makes sense that we should be able to select a subset of it.\n",
    "\n",
    "Similarly, a `DataFrameEncoder` should allow getting `SeriesEncoder` from individual columns.\n",
    "\n",
    "For others, such as `FloatEncoder` or `TensorEncoder`, it shouldn't matter. Should they be allowed to \"pass through\" the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea: Encode the indices as well and look up the encoded index data!\n",
    "\n",
    "Not sure about ramifications: If one encodes to float, rounding errors might lead to faulty lookups!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea: Do we really need / want index encoding?\n",
    "\n",
    "Shouldn't Series encoding not be enough?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea: Change DataFrameEncoder's behaviour\n",
    "\n",
    "- By default, map DataFrame -> DataFrame\n",
    "\n",
    "Add either an option, or an additional encoder that splits a DataFrame by columns / index into multiple sub-frames / series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea: TupleDataset  / MappingDataset\n",
    "\n",
    "`TupleDataset` basically dups `TensorDataset`, but doesn't require Tensors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea: TupleSampler / MappingSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⟹ Algebraify all the data structures!!! ⟸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Slicing - The details\n",
    "\n",
    "1. Slicing ChainedEncoder\n",
    "    - try from the top until implemented, then return new Chained-Encoder representing the slice\n",
    "        - New object problematic, since it won't get updated when the original changes?\n",
    "        - What happens to the slice if we re-fit the original? Does the slice change as well?\n",
    "            - Since the content are references to the same objects, it should work!\n",
    "        - What happens when we refit a slice, does the original change?\n",
    "2. Slicing BaseEncoder\n",
    "    - Either: Do nothing, or raise error\n",
    "3. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
