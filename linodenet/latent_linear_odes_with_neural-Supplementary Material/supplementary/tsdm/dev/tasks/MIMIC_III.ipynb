{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC_III Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.samples = [\n",
    "            {\"x1\": torch.arange(10), \"x2\": torch.arange(10, 20)},\n",
    "            {\"x1\": torch.arange(10), \"x2\": torch.arange(10, 20)},\n",
    "            {\"x1\": torch.arange(10), \"x2\": torch.arange(10, 20)},\n",
    "            {\"x1\": torch.arange(10), \"x2\": torch.arange(10, 20)},\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "\n",
    "dataset = MyDataset()\n",
    "loader = DataLoader(dataset, batch_size=2, num_workers=0)\n",
    "\n",
    "for batch in loader:\n",
    "    x1 = batch[\"x1\"]\n",
    "    print(x1)\n",
    "    print(x1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable, Iterator, Mapping, Sequence\n",
    "from dataclasses import dataclass\n",
    "from functools import cached_property\n",
    "from typing import Any, NamedTuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Index, MultiIndex\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import Tensor, nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tsdm.datasets import USHCN_SmallChunkedSporadic\n",
    "from tsdm.tasks.base import BaseTask\n",
    "from tsdm.utils.strings import repr_namedtuple\n",
    "\n",
    "\n",
    "class Inputs(NamedTuple):\n",
    "    r\"\"\"A single sample of the data.\"\"\"\n",
    "\n",
    "    t: Tensor\n",
    "    x: Tensor\n",
    "    t_target: Tensor\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return repr_namedtuple(self, recursive=False)\n",
    "\n",
    "\n",
    "class Sample(NamedTuple):\n",
    "    r\"\"\"A single sample of the data.\"\"\"\n",
    "\n",
    "    key: int\n",
    "    inputs: Inputs\n",
    "    targets: Tensor\n",
    "    originals: tuple[Tensor, Tensor]\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return repr_namedtuple(self, recursive=False)\n",
    "\n",
    "\n",
    "class Batch(NamedTuple):\n",
    "    r\"\"\"A single sample of the data.\"\"\"\n",
    "\n",
    "    T: Tensor  # B×N: the timestamps.\n",
    "    X: Tensor  # B×N×D: the observations.\n",
    "    Y: Tensor  # B×K×D: the target values.\n",
    "    M: Tensor  # B×N: which t correspond to targets.\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return repr_namedtuple(self, recursive=False)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TaskDataset(torch.utils.data.Dataset):\n",
    "    r\"\"\"Wrapper for creating samples of the dataset.\"\"\"\n",
    "\n",
    "    tensors: dict[int, tuple[Tensor, Tensor]]\n",
    "    observation_time: float\n",
    "    prediction_steps: int\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        r\"\"\"Return the number of samples in the dataset.\"\"\"\n",
    "        return len(self.tensors)\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        r\"\"\"Return an iterator over the dataset.\"\"\"\n",
    "        return iter(self.tensors)\n",
    "\n",
    "    def __getitem__(self, key: int) -> Sample:\n",
    "        t, x = self.tensors[key]\n",
    "        observation_mask = t <= self.observation_time\n",
    "        first_target = observation_mask.sum()\n",
    "        target_mask = slice(first_target, first_target + self.prediction_steps)\n",
    "        return Sample(\n",
    "            key=key,\n",
    "            inputs=Inputs(t[observation_mask], x[observation_mask], t[target_mask]),\n",
    "            targets=x[target_mask],\n",
    "            originals=(t, x),\n",
    "        )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}\"\n",
    "\n",
    "\n",
    "def my_collate(batch: list[Sample]) -> Batch:\n",
    "    r\"\"\"Collate tensors into batch.\"\"\"\n",
    "    t_list: list[Tensor] = []\n",
    "    x_list: list[Tensor] = []\n",
    "    m_list: list[Tensor] = []\n",
    "    y_list: list[Tensor] = []\n",
    "\n",
    "    for sample in batch:\n",
    "        t, x, t_target = sample.inputs\n",
    "        mask = torch.cat(\n",
    "            (\n",
    "                torch.zeros_like(t, dtype=torch.bool),\n",
    "                torch.ones_like(t_target, dtype=torch.bool),\n",
    "            )\n",
    "        )\n",
    "        x_padder = torch.full((t_target.shape[0], x.shape[-1]), fill_value=torch.nan)\n",
    "        time = torch.cat((t, t_target))\n",
    "        values = torch.cat((x, x_padder))\n",
    "        idx = torch.argsort(time)\n",
    "        t_list.append(time[idx])\n",
    "        x_list.append(values[idx])\n",
    "        m_list.append(mask[idx])\n",
    "        y_list.append(sample.targets)\n",
    "\n",
    "    T = pad_sequence(t_list, batch_first=True, padding_value=torch.nan).squeeze()\n",
    "    X = pad_sequence(x_list, batch_first=True, padding_value=torch.nan).squeeze()\n",
    "    Y = pad_sequence(y_list, batch_first=True, padding_value=torch.nan).squeeze()\n",
    "    M = pad_sequence(m_list, batch_first=True, padding_value=False).squeeze()\n",
    "\n",
    "    return Batch(T, X, Y, M)\n",
    "\n",
    "\n",
    "class MIMIC_DeBrouwer(BaseTask):\n",
    "    r\"\"\"Preprocessed subset of the USHCN climate dataset used by De Brouwer et. al.\n",
    "\n",
    "    Evaluation Protocol\n",
    "    -------------------\n",
    "\n",
    "        5.3Climate forecast\n",
    "\n",
    "        From short-term weather forecast to long-range prediction or assessment of systemic\n",
    "        changes, such as global warming, climatic data has always been a popular application for\n",
    "        time-series analysis. This data is often considered to be regularly sampled over long\n",
    "        periods of time, which facilitates their statistical analysis. Yet, this assumption does\n",
    "        not usually hold in practice. Missing data are a problem that is repeatedly encountered in\n",
    "        climate research because of, among others, measurement errors, sensor failure, or faulty\n",
    "        data acquisition. The actual data is then sporadic and researchers usually resort to\n",
    "        imputation before statistical analysis (Junninen et al., 2004; Schneider, 2001).\n",
    "\n",
    "        We use the publicly available United State Historical Climatology Network (USHCN) daily\n",
    "        data set (Menne et al.), which contains measurements of 5 climate variables\n",
    "        (daily temperatures, precipitation, and snow) over 150 years for 1,218 meteorological\n",
    "        stations scattered over the United States. We selected a subset of 1,114 stations and an\n",
    "        observation window of 4 years (between 1996 and 2000). To make the time series sporadic, we\n",
    "        subsample the data such that each station has an average of around 60 observations over\n",
    "        those 4 years. Appendix L contains additional details regarding this procedure.\n",
    "        The task is then to predict the next 3 measurements after the first 3 years of observation.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    - | `GRU-ODE-Bayes: Continuous Modeling of Sporadically-Observed Time Series\n",
    "        <https://proceedings.neurips.cc/paper/2019/hash/455cb2657aaa59e32fad80cb0b65b9dc-Abstract.html>`_\n",
    "      | De Brouwer, Edward and Simm, Jaak and Arany, Adam and Moreau, Yves\n",
    "      | `Advances in Neural Information Processing Systems 2019\n",
    "        <https://proceedings.neurips.cc/paper/2019>`_\n",
    "    \"\"\"\n",
    "\n",
    "    observation_time = 75\n",
    "    prediction_steps = 3\n",
    "    num_folds = 5\n",
    "    seed = 432\n",
    "    test_size = 0.1\n",
    "    valid_size = 0.2\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.IDs = self.dataset.reset_index()[\"UNIQUE_ID\"].unique()\n",
    "\n",
    "    @cached_property\n",
    "    def dataset(self) -> DataFrame:\n",
    "        r\"\"\"Load the dataset.\"\"\"\n",
    "        return MIMIC_III().observations\n",
    "\n",
    "    @cached_property\n",
    "    def folds(self) -> list[dict[str, Sequence[int]]]:\n",
    "        r\"\"\"Create the folds.\"\"\"\n",
    "        num_folds = 5\n",
    "        folds = []\n",
    "        np.random.seed(self.seed)\n",
    "        for _ in range(num_folds):\n",
    "            train_idx, test_idx = train_test_split(self.IDs, test_size=self.test_size)\n",
    "            train_idx, valid_idx = train_test_split(\n",
    "                train_idx, test_size=self.valid_size\n",
    "            )\n",
    "            folds.append(\n",
    "                {\n",
    "                    \"train\": train_idx,\n",
    "                    \"valid\": valid_idx,\n",
    "                    \"test\": test_idx,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return folds\n",
    "\n",
    "    @cached_property\n",
    "    def split_idx(self):\n",
    "        r\"\"\"Create the split index.\"\"\"\n",
    "        fold_idx = Index(list(range(len(self.folds))), name=\"fold\")\n",
    "        splits = DataFrame(index=self.IDs, columns=fold_idx, dtype=\"string\")\n",
    "\n",
    "        for k in range(self.num_folds):\n",
    "            for key, split in self.folds[k].items():\n",
    "                mask = splits.index.isin(split)\n",
    "                splits[k] = splits[k].where(\n",
    "                    ~mask, key\n",
    "                )  # where cond is false is replaces with key\n",
    "        return splits\n",
    "\n",
    "    @cached_property\n",
    "    def split_idx_sparse(self) -> DataFrame:\n",
    "        r\"\"\"Return sparse table with indices for each split.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        DataFrame[bool]\n",
    "        \"\"\"\n",
    "        df = self.split_idx\n",
    "        columns = df.columns\n",
    "\n",
    "        # get categoricals\n",
    "        categories = {\n",
    "            col: df[col].astype(\"category\").dtype.categories for col in columns\n",
    "        }\n",
    "\n",
    "        if isinstance(df.columns, MultiIndex):\n",
    "            index_tuples = [\n",
    "                (*col, cat)\n",
    "                for col, cats in zip(columns, categories)\n",
    "                for cat in categories[col]\n",
    "            ]\n",
    "            names = df.columns.names + [\"partition\"]\n",
    "        else:\n",
    "            index_tuples = [\n",
    "                (col, cat)\n",
    "                for col, cats in zip(columns, categories)\n",
    "                for cat in categories[col]\n",
    "            ]\n",
    "            names = [df.columns.name, \"partition\"]\n",
    "\n",
    "        new_columns = MultiIndex.from_tuples(index_tuples, names=names)\n",
    "        result = DataFrame(index=df.index, columns=new_columns, dtype=bool)\n",
    "\n",
    "        if isinstance(df.columns, MultiIndex):\n",
    "            for col in new_columns:\n",
    "                result[col] = df[col[:-1]] == col[-1]\n",
    "        else:\n",
    "            for col in new_columns:\n",
    "                result[col] = df[col[0]] == col[-1]\n",
    "\n",
    "        return result\n",
    "\n",
    "    @cached_property\n",
    "    def test_metric(self) -> Callable[[Tensor, Tensor], Tensor]:\n",
    "        r\"\"\"The test metric.\"\"\"\n",
    "        return nn.MSELoss()\n",
    "\n",
    "    @cached_property\n",
    "    def splits(self) -> Mapping:\n",
    "        r\"\"\"Create the splits.\"\"\"\n",
    "        splits = {}\n",
    "        for key in self.index:\n",
    "            mask = self.split_idx_sparse[key]\n",
    "            ids = self.split_idx_sparse.index[mask]\n",
    "            splits[key] = self.dataset.loc[ids]\n",
    "        return splits\n",
    "\n",
    "    @cached_property\n",
    "    def index(self) -> MultiIndex:\n",
    "        r\"\"\"Create the index.\"\"\"\n",
    "        return self.split_idx_sparse.columns\n",
    "\n",
    "    @cached_property\n",
    "    def tensors(self) -> Mapping:\n",
    "        r\"\"\"Tensor dictionary.\"\"\"\n",
    "        tensors = {}\n",
    "        for _id in self.IDs:\n",
    "            s = self.dataset.loc[_id]\n",
    "            t = torch.tensor(s.index.values, dtype=torch.float32)\n",
    "            x = torch.tensor(s.values, dtype=torch.float32)\n",
    "            tensors[_id] = (t, x)\n",
    "        return tensors\n",
    "\n",
    "    def get_dataloader(\n",
    "        self, key: tuple[int, str], /, **dataloader_kwargs: Any\n",
    "    ) -> DataLoader:\n",
    "        \"\"\"Return the dataloader for the given key.\"\"\"\n",
    "        fold, partition = key\n",
    "        fold_idx = self.folds[fold][partition]\n",
    "        dataset = TaskDataset(\n",
    "            [val for idx, val in self.tensors.items() if idx in fold_idx],\n",
    "            observation_time=self.observation_time,\n",
    "            prediction_steps=self.prediction_steps,\n",
    "        )\n",
    "\n",
    "        kwargs: dict[str, Any] = {\"collate_fn\": lambda *x: x} | dataloader_kwargs\n",
    "        return DataLoader(dataset, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = MIMIC_DeBrouwer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task.tensors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader = task.get_dataloader((0, \"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(dloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsdm.encoders import TripletDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = MIMIC_III().observations\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = TripletDecoder(value_name=\"VALUENORM\", var_name=\"LABEL_CODE\")\n",
    "enc.fit(ts)\n",
    "encoded = enc.encode(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.notna(encoded).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded[\"TIME_STAMP\"] = encoded[\"TIME_STAMP\"] * pd.Timedelta(\"6m\")\n",
    "\n",
    "IDS = encoded[\"UNIQUE_ID\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = encoded[encoded[\"TIME_STAMP\"] <= pd.Timedelta(\"48h\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = encoded.set_index([\"UNIQUE_ID\", \"TIME_STAMP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, df in encoded.groupby(\"UNIQUE_ID\"):\n",
    "\n",
    "    display(df)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.groupby(\"UNIQUE_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "250 * 60 / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = encoded.reset_index()[\"TIME_STAMP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = ts.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
