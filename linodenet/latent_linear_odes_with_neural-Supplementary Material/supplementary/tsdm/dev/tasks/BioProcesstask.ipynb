{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioProcess Task\n",
    "\n",
    "We update the BioProcess task:\n",
    "\n",
    "- 5 fold cross-validation into train/valid/test\n",
    "- On a single Time Series, the task is to forecast over a sliding window. The number of datapoints in each slice can vary due to irregular sampling\n",
    "\n",
    "\n",
    "A timeseries dataset consists of 4 components:\n",
    "\n",
    "1. Observational Data $(ğ­^\\text{obs},ğ—)$ which is available in the observation horizon\n",
    "2. Target Data $(ğ­^\\text{pred},ğ˜)$, which is values we should predict in the forecasting horizon. <br> We distinguish 2 subcases:\n",
    "    1. Autoregressive case: If $ğ˜$ are also observational variables\n",
    "    2. Non-Autoregressive case: If $ğ˜$ is not observed / provided over the observation horizon\n",
    "    More specifically, we can split up $ğ˜$ into $(ğ­^\\text{pred},ğ˜^\\text{A})$ and $(ğ­^\\text{pred},ğ˜^\\text{B})$ for the autoregressive and non-autoregressive part\n",
    "3. Covariate Data $(ğ­^\\text{cov},ğ”)$\n",
    "4. Time-Independent Metadata $ğ“œ$\n",
    "\n",
    "Summa summarum a timeseries is described by a 4/5-tuple \n",
    "\n",
    "$$ ğ“£ğ“¢ = \\Big( (ğ­^\\text{obs},ğ—)ï¼Œ(ğ­^\\text{cov},ğ”)ï¼Œ(ğ­^\\text{pred},ğ˜)ï¼Œğ“œ\\Big)\n",
    "\\qquad ğ“£ğ“¢ =  \\Big( (ğ­^\\text{obs},ğ—)ï¼Œ(ğ­^\\text{cov},ğ”)ï¼Œ(ğ­^\\text{auto},ğ˜^\\text{auto})ï¼Œ(ğ­^\\text{pred}, ğ˜^\\text{pred})ï¼Œğ“œ\\Big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shorten the writing, by some abuse of notation we write $ğ—_ğ­â‰”(ğ­^\\text{obs},ğ—)$ and similarly for the other variables\n",
    "\n",
    "$$ ğ“£ğ“¢ = \\Big(ğ—_ğ­ï¼Œğ”_ğ­ï¼Œğ˜_ğ­ï¼Œğ“œ\\Big)\n",
    "\\qquad ğ“£ğ“¢ =  \\Big(ğ—_ğ­ï¼Œğ”_ğ­ï¼Œğ˜_ğ­=(ğ˜_ğ­^\\text{auto}ï¼Œğ˜_ğ­^\\text{pred})ï¼Œğ“œ\\Big)\n",
    "$$\n",
    "\n",
    "Further, we introduce the notations:\n",
    "\n",
    "- $ğ—_{S}â‰”\\{ (táµ¢, X_{táµ¢}) âˆ£ táµ¢âˆˆS\\}$ for any set $SâŠ‚ğ“£$, in particular intervals:\n",
    "    - $ğ—_{[t,t']}â‰”\\{ (táµ¢, X_{táµ¢}) âˆ£ tâ‰¤táµ¢â‰¤t'\\}$ (closed interval)\n",
    "    - $ğ—_{[t,t')}â‰”\\{ (táµ¢, X_{táµ¢}) âˆ£ tâ‰¤táµ¢<'\\}$ (half-open interval)\n",
    "- We allow set addition $t+S â‰” \\{t+sâˆ£sâˆˆS\\}$, hence\n",
    "    - $ğ—_{[t, t'] + âˆ†t}â‰”\\{ (táµ¢, X_{táµ¢}) âˆ£ (t+âˆ†t) â‰¤ táµ¢â‰¤ (t'+âˆ†t)\\}$\n",
    "- $ğ—_{â‰¤t}â‰” \\{ (táµ¢, X_{táµ¢}) âˆ£ táµ¢â‰¤t\\}$, and analogously $ğ—_{<t}$, $ğ—_{>t}$, $ğ—_{â‰¥t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is, given an observation horizon $I^\\text{obs}$ and a forecasting horizon $I^\\text{pred}$, <br>\n",
    "typically $I^\\text{obs} = [t, t+âˆ†t^\\text{obs}) = [t, s)$, $I^\\text{pred}= [t+âˆ†t^\\text{obs}, t+âˆ†t^\\text{obs}+âˆ†t^\\text{pred}) = [s,r)$\n",
    "\n",
    "\n",
    "$$â„“(ğ˜_{J}, \\hat{ğ˜}(ğ—_{I}ï¼Œğ”_{I+J}ï¼Œğ˜_{I}^\\text{auto}ï¼Œğ“œ))$$\n",
    "\n",
    "We define initial intervals: \n",
    "- Initial Observation interval $I=[t_S, t_O)=[t_\\text{start}, t_\\text{start}+âˆ†t^\\text{obs})$\n",
    "- Initial Forecasting interval $J=[t_O, t_P)=[t_\\text{start}+âˆ†t^\\text{obs}, t_\\text{start}+âˆ†t^\\text{obs}+âˆ†t^\\text{pred})$\n",
    "- Initial Complete Interval $K=IâˆªJ=[t_S, t_p) = [t_\\text{start}, t_\\text{start}+âˆ†t^\\text{obs}+âˆ†t^\\text{pred})$\n",
    "\n",
    "Given a stride, i.e. a time delta $âˆ†t$, we define the set of all integer multiples of $âˆ†t$ that can be added to $tâ‚€$ such that it stays inside the bounds of $[t_\\min, t_\\max]$:\n",
    "\n",
    "$$W(t_\\min, t_\\max, âˆ†t, tâ‚€) â‰” \\{kâ‹…âˆ†t âˆ£ kâˆˆâ„¤ âˆ§ (tâ‚€+kâ‹…Î”t) âˆˆ [tâ‚˜áµ¢â‚™ï¼Œtâ‚˜â‚â‚“]\\}$$\n",
    "\n",
    "Moreover, let $W^+$ and $W^-$ be the sets that only take non-negative / non-positive $k$.\n",
    "\n",
    "Then $G_ğ“£â‰”W^+(T_\\min, T_\\max, âˆ†s, t_P)$ is the set of all increments we can apply to $I$ and $J$ such that they stay within the bounds\n",
    "\n",
    "\\begin{equation}\n",
    "ğ“›(Î¸) = \n",
    "\\frac{1}{|G_ğ“£|}\\sum_{âˆ†tâˆˆG_ğ“£} \n",
    "\\frac{1}{|ğ˜_{J+âˆ†t}|}\\sum_{t,y âˆˆ ğ˜_{J+âˆ†t}} \n",
    "â„“\\big( y, \\hat{y}_Î¸(t âˆ£ ğ—_{I+âˆ†t}ï¼Œğ”_{K+âˆ†t}ï¼Œğ˜_{I+âˆ†t}^\\text{auto}ï¼Œğ“œ)\\big)\n",
    "\\end{equation}\n",
    "\n",
    "Note that in order to achieve good training across all channels, even if they are sampled at wastely different rates, we normalize with respect to the observation frequency.\n",
    "\n",
    "$$\n",
    "\\widetilde{\\sum} =  \\frac{\\sum_i m_t â„“(y_t, \\hat{y}_y)}{\\sum m_t}\n",
    "$$\n",
    "\n",
    "Note that, in order to avoid division by zero errors, we formally set the sum to zero if $âˆ‘_t m_t = 0$. In practice this is achieved by simply applying a `nan-mean` function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Error Metric: Weighted L2 / L1 / ND score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'  # always print last expr.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=4, floatmode=\"fixed\", suppress=True)\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold, ShuffleSplit\n",
    "from numpy import array_split\n",
    "from numpy import random as rng\n",
    "from pandas import Series, Index, DataFrame\n",
    "from tsdm.utils.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsdm.datasets import KIWI_RUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = KIWI_RUNS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ds.metadata.drop([355, 482])\n",
    "groups = md.groupby([\"run_id\", \"color\"], sort=False).ngroup()\n",
    "folds = folds_from_groups(groups, train=7, valid=1, test=2)\n",
    "assert all(sum([fold[\"test\"] for fold in folds]) == 1)\n",
    "assert all(sum([fold[\"valid\"] for fold in folds]) <= 1)\n",
    "splits = folds_as_frame(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'  # always print last expr.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "from functools import cached_property\n",
    "from itertools import product\n",
    "from typing import Any, Literal, NamedTuple, Optional\n",
    "\n",
    "import torch\n",
    "from pandas import DataFrame, Series\n",
    "from torch import Tensor, jit\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tsdm.datasets import KIWI_RUNS\n",
    "from tsdm.encoders import BaseEncoder\n",
    "from tsdm.metrics import WRMSE\n",
    "from tsdm.random.samplers import HierarchicalSampler, SequenceSampler\n",
    "from tsdm.tasks.base import BaseTask\n",
    "from tsdm.utils.strings import repr_namedtuple\n",
    "from tsdm.utils.data import MappingDataset, TimeSeriesDataset\n",
    "from tsdm.utils.data import folds_from_groups, folds_as_frame\n",
    "\n",
    "\n",
    "class Sample(NamedTuple):\n",
    "    r\"\"\"A sample of the data.\"\"\"\n",
    "\n",
    "    key: tuple[tuple[int, int], slice]\n",
    "    inputs: tuple[DataFrame, DataFrame]\n",
    "    targets: float\n",
    "    originals: Optional[tuple[DataFrame, DataFrame]] = None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return repr_namedtuple(self, recursive=1)\n",
    "\n",
    "\n",
    "class Kiwi_BioProcessTask(BaseTask):\n",
    "    r\"\"\"A collection of bioreactor runs.\n",
    "\n",
    "    For this task we do several simplifications\n",
    "\n",
    "    - drop run_id 355\n",
    "    - drop almost all metadata\n",
    "    - restrict timepoints to start_time & end_time given in metadata.\n",
    "\n",
    "    - timeseries for each run_id and experiment_id\n",
    "    - metadata for each run_id and experiment_id\n",
    "\n",
    "    When first do a train/test split.\n",
    "    Then the goal is to learn a model in a multi-task fashion on all the ts.\n",
    "\n",
    "    To train, we sample\n",
    "    1. random TS from the dataset\n",
    "    2. random snippets from the sampled TS\n",
    "\n",
    "    Questions:\n",
    "    - Should each batch contain only snippets form a single TS, or is there merit to sampling\n",
    "    snippets from multiple TS in each batch?\n",
    "\n",
    "    Divide 'Glucose' by 10, 'OD600' by 20, 'DOT' by 100, 'Base' by 200, then use RMSE.\n",
    "    \"\"\"\n",
    "\n",
    "    index: list[tuple[int, str]] = list(product(range(5), (\"train\", \"valid\", \"test\")))\n",
    "    r\"\"\"Available index.\"\"\"\n",
    "    KeyType = tuple[Literal[0, 1, 2, 3, 4], Literal[\"train\", \"test\"]]\n",
    "    r\"\"\"Type Hint for Keys.\"\"\"\n",
    "    timeseries: DataFrame\n",
    "    r\"\"\"The whole timeseries data.\"\"\"\n",
    "    metadata: DataFrame\n",
    "    r\"\"\"The metadata.\"\"\"\n",
    "    observation_horizon: int = 96\n",
    "    r\"\"\"The number of datapoints observed during prediction.\"\"\"\n",
    "    forecasting_horizon: int = 24\n",
    "    r\"\"\"The number of datapoints the model should forecast.\"\"\"\n",
    "    preprocessor: BaseEncoder\n",
    "    r\"\"\"Encoder for the observations.\"\"\"\n",
    "    controls: Series\n",
    "    r\"\"\"The control variables.\"\"\"\n",
    "    targets: Series\n",
    "    r\"\"\"The target variables.\"\"\"\n",
    "    observables: Series\n",
    "    r\"\"\"The observables variables.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        forecasting_horizon: int = 24,\n",
    "        observation_horizon: int = 96,\n",
    "    ):\n",
    "        self.forecasting_horizon = forecasting_horizon\n",
    "        self.observation_horizon = observation_horizon\n",
    "        self.horizon = self.observation_horizon + self.forecasting_horizon\n",
    "\n",
    "        self.timeseries = ts = self.dataset.timeseries\n",
    "        self.metadata = self.dataset.metadata\n",
    "        self.units = self.dataset.units\n",
    "\n",
    "        self.targets = targets = Series([\"Base\", \"DOT\", \"Glucose\", \"OD600\"])\n",
    "        self.targets.index = self.targets.apply(ts.columns.get_loc)\n",
    "\n",
    "        self.controls = controls = Series(\n",
    "            [\n",
    "                \"Cumulated_feed_volume_glucose\",\n",
    "                \"Cumulated_feed_volume_medium\",\n",
    "                \"InducerConcentration\",\n",
    "                \"StirringSpeed\",\n",
    "                \"Flow_Air\",\n",
    "                \"Temperature\",\n",
    "                \"Probe_Volume\",\n",
    "            ]\n",
    "        )\n",
    "        controls.index = controls.apply(ts.columns.get_loc)\n",
    "\n",
    "        self.observables = observables = Series(\n",
    "            [\n",
    "                \"Base\",\n",
    "                \"DOT\",\n",
    "                \"Glucose\",\n",
    "                \"OD600\",\n",
    "                \"Acetate\",\n",
    "                \"Fluo_GFP\",\n",
    "                \"Volume\",\n",
    "                \"pH\",\n",
    "            ]\n",
    "        )\n",
    "        observables.index = observables.apply(ts.columns.get_loc)\n",
    "\n",
    "        assert (\n",
    "            set(controls.values) | set(targets.values) | set(observables.values)\n",
    "        ) == set(ts.columns)\n",
    "\n",
    "    @cached_property\n",
    "    def test_metric(self) -> Callable[..., Tensor]:\n",
    "        r\"\"\"The metric to be used for evaluation.\"\"\"\n",
    "        ts = self.timeseries\n",
    "        weights = DataFrame.from_dict(\n",
    "            {\n",
    "                \"Base\": 200,\n",
    "                \"DOT\": 100,\n",
    "                \"Glucose\": 10,\n",
    "                \"OD600\": 20,\n",
    "            },\n",
    "            orient=\"index\",\n",
    "            columns=[\"inverse_weight\"],\n",
    "        )\n",
    "        weights[\"col_index\"] = weights.index.map(lambda x: (ts.columns == x).argmax())\n",
    "        weights[\"weight\"] = 1 / weights[\"inverse_weight\"]\n",
    "        weights[\"normalized\"] = weights[\"weight\"] / weights[\"weight\"].sum()\n",
    "        weights.index.name = \"col\"\n",
    "        w = torch.tensor(weights[\"weight\"])\n",
    "        return jit.script(WRMSE(w))\n",
    "\n",
    "    @cached_property\n",
    "    def dataset(self) -> KIWI_RUNS:\n",
    "        r\"\"\"Return the cached dataset.\"\"\"\n",
    "        dataset = KIWI_RUNS()\n",
    "        dataset.metadata.drop([355, 482], inplace=True)\n",
    "        dataset.timeseries.drop([355, 482], inplace=True)\n",
    "        return dataset\n",
    "\n",
    "    @cached_property\n",
    "    def folds(self) -> DataFrame:\n",
    "        r\"\"\"Return the folds.\"\"\"\n",
    "        md = self.dataset.metadata\n",
    "        groups = md.groupby([\"run_id\", \"color\"], sort=False).ngroup()\n",
    "        folds = folds_from_groups(\n",
    "            groups, seed=2022, num_folds=5, train=7, valid=1, test=2\n",
    "        )\n",
    "        return folds_as_frame(folds)\n",
    "\n",
    "    @cached_property\n",
    "    def splits(self) -> dict[Any, tuple[DataFrame, DataFrame]]:\n",
    "        r\"\"\"Return a subset of the data corresponding to the split.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[DataFrame, DataFrame]\n",
    "        \"\"\"\n",
    "        splits = {}\n",
    "        for key in self.index:\n",
    "            assert key in self.index, f\"Wrong {key=}. Only {self.index} work.\"\n",
    "            split, data_part = key\n",
    "\n",
    "            mask = self.folds[split] == data_part\n",
    "            idx = self.folds[split][mask].index\n",
    "            timeseries = self.timeseries.reset_index(level=2).loc[idx]\n",
    "            timeseries = timeseries.set_index(\"measurement_time\", append=True)\n",
    "            metadata = self.metadata.loc[idx]\n",
    "            splits[key] = (timeseries, metadata)\n",
    "        return splits\n",
    "\n",
    "    @cached_property\n",
    "    def dataloader_kwargs(self) -> dict:\n",
    "        r\"\"\"Return the kwargs for the dataloader.\"\"\"\n",
    "        return {\n",
    "            \"batch_size\": 1,\n",
    "            \"shuffle\": False,\n",
    "            \"sampler\": None,\n",
    "            \"batch_sampler\": None,\n",
    "            \"num_workers\": 0,\n",
    "            \"collate_fn\": lambda *x: x,\n",
    "            \"pin_memory\": False,\n",
    "            \"drop_last\": False,\n",
    "            \"timeout\": 0,\n",
    "            \"worker_init_fn\": None,\n",
    "            \"prefetch_factor\": 2,\n",
    "            \"persistent_workers\": False,\n",
    "        }\n",
    "\n",
    "    def get_dataloader(\n",
    "        self, key: KeyType, /, shuffle: bool = False, **dataloader_kwargs: Any\n",
    "    ) -> DataLoader:\n",
    "        r\"\"\"Return a dataloader for the given split.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        key: KeyType,\n",
    "        shuffle: bool, default False\n",
    "        dataloader_kwargs: Any,\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        DataLoader\n",
    "        \"\"\"\n",
    "        # Construct the dataset object\n",
    "        ts, md = self.splits[key]\n",
    "        dataset = _Dataset(\n",
    "            ts,\n",
    "            md,\n",
    "            observables=self.observables.index,\n",
    "            observation_horizon=self.observation_horizon,\n",
    "            targets=self.targets.index,\n",
    "        )\n",
    "\n",
    "        TSDs = {}\n",
    "        for idx in md.index:\n",
    "            TSDs[idx] = TimeSeriesDataset(\n",
    "                ts.loc[idx],\n",
    "                metadata=md.loc[idx],\n",
    "            )\n",
    "        DS = MappingDataset(TSDs)\n",
    "\n",
    "        # construct the sampler\n",
    "        subsamplers = {\n",
    "            key: SequenceSampler(ds, seq_len=self.horizon, shuffle=shuffle)\n",
    "            for key, ds in DS.items()\n",
    "        }\n",
    "        sampler = HierarchicalSampler(DS, subsamplers, shuffle=shuffle)\n",
    "\n",
    "        # construct the dataloader\n",
    "        kwargs: dict[str, Any] = {\"collate_fn\": lambda *x: x} | dataloader_kwargs\n",
    "        return DataLoader(dataset, sampler=sampler, **kwargs)\n",
    "\n",
    "\n",
    "class _Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ts, md, *, observables, targets, observation_horizon):\n",
    "        super().__init__()\n",
    "        self.timeseries = ts\n",
    "        self.metadata = md\n",
    "        self.observables = observables\n",
    "        self.targets = targets\n",
    "        self.observation_horizon = observation_horizon\n",
    "\n",
    "    def __getitem__(self, item: tuple[tuple[int, int], slice]) -> Sample:\n",
    "        key, slc = item\n",
    "        ts = self.timeseries.loc[key].iloc[slc].copy(deep=True)\n",
    "        md = self.metadata.loc[key].copy(deep=True)\n",
    "        originals = (ts.copy(deep=True), md.copy(deep=True))\n",
    "        targets = ts.iloc[self.observation_horizon :, self.targets].copy(deep=True)\n",
    "        ts.iloc[self.observation_horizon :, self.targets] = float(\"nan\")\n",
    "        ts.iloc[self.observation_horizon :, self.observables] = float(\"nan\")\n",
    "        return Sample(key=item, inputs=(ts, md), targets=targets, originals=originals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = Kiwi_BioProcessTask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.splits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = (0, \"train\")\n",
    "shuffle = False\n",
    "\n",
    "####################\n",
    "\n",
    "\n",
    "ts, md = self.splits[key]\n",
    "dataset = _Dataset(\n",
    "    ts,\n",
    "    md,\n",
    "    observables=self.observables.index,\n",
    "    observation_horizon=self.observation_horizon,\n",
    "    targets=self.targets.index,\n",
    ")\n",
    "\n",
    "TSDs = {}\n",
    "for idx in md.index:\n",
    "    TSDs[idx] = TimeSeriesDataset(\n",
    "        ts.loc[idx],\n",
    "        metadata=md.loc[idx],\n",
    "    )\n",
    "DS = MappingDataset(TSDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_, ds = next(iter(DS.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SequenceSampler(ds, seq_len=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the sampler\n",
    "subsamplers = {\n",
    "    key_: SequenceSampler(ds, seq_len=self.horizon, shuffle=shuffle)\n",
    "    for key_, ds in DS.items()\n",
    "}\n",
    "sampler = HierarchicalSampler(DS, subsamplers, shuffle=shuffle)\n",
    "\n",
    "# construct the dataloader\n",
    "kwargs: dict[str, Any] = {\"collate_fn\": lambda *x: x} | dataloader_kwargs\n",
    "DataLoader(dataset, sampler=sampler, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
