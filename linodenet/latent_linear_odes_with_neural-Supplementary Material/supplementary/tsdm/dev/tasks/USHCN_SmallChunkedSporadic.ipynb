{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USHCN task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'  # always print last expr.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Index\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How splits should look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsdm.tasks import KIWI_RUNS_TASK\n",
    "\n",
    "task = KIWI_RUNS_TASK()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task.splits  # dict that holds the actual data\n",
    "task.split_idx  # dense form\n",
    "task.split_idx_sparse  # sparse splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsdm\n",
    "\n",
    "USHCN = tsdm.datasets.USHCN_SmallChunkedSporadic()\n",
    "ds = USHCN.dataset\n",
    "IDX = ds.index.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Mapping, Sequence\n",
    "from functools import cached_property\n",
    "from typing import Any\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from pandas import DataFrame, MultiIndex\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tsdm.datasets import USHCN_SmallChunkedSporadic\n",
    "from tsdm.tasks import BaseTask\n",
    "\n",
    "\n",
    "class USHCN(BaseTask):\n",
    "    \"\"\"TODO: resale time minmax.\"\"\"\n",
    "    \n",
    "    observation_time = 150\n",
    "    prediction_steps = 3\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.IDs = self.dataset.reset_index()[\"ID\"].unique()\n",
    "\n",
    "    @cached_property\n",
    "    def dataset(self) -> DataFrame:\n",
    "        return USHCN_SmallChunkedSporadic().dataset\n",
    "\n",
    "    @cached_property\n",
    "    def folds(self) -> list[dict[int, Sequence]]:\n",
    "        num_folds = 5\n",
    "        folds = []\n",
    "        np.random.seed(432)\n",
    "        for fold in range(num_folds):\n",
    "            train_idx, test_idx = train_test_split(self.IDs, test_size=0.1)\n",
    "            train_idx, valid_idx = train_test_split(train_idx, test_size=0.2)\n",
    "            folds.append(\n",
    "                {\n",
    "                    \"train\": train_idx,\n",
    "                    \"valid\": valid_idx,\n",
    "                    \"test\": test_idx,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return folds\n",
    "\n",
    "    @cached_property\n",
    "    def split_idx(self):\n",
    "        fold_idx = Index(list(range(len(self.folds))), name=\"fold\")\n",
    "\n",
    "        splits = DataFrame(index=self.IDs, columns=fold_idx, dtype=\"string\")\n",
    "\n",
    "        for k in range(num_folds):\n",
    "            for key, split in self.folds[k].items():\n",
    "                mask = splits.index.isin(split)\n",
    "                splits[k] = splits[k].where(\n",
    "                    ~mask, key\n",
    "                )  # where cond is false is replaces with key\n",
    "        return splits\n",
    "\n",
    "    @cached_property\n",
    "    def split_idx_sparse(self) -> DataFrame:\n",
    "        r\"\"\"Return sparse table with indices for each split.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        DataFrame[bool]\n",
    "        \"\"\"\n",
    "        df = self.split_idx\n",
    "        columns = df.columns\n",
    "\n",
    "        # get categoricals\n",
    "        categories = {\n",
    "            col: df[col].astype(\"category\").dtype.categories for col in columns\n",
    "        }\n",
    "\n",
    "        if isinstance(df.columns, MultiIndex):\n",
    "            index_tuples = [\n",
    "                (*col, cat)\n",
    "                for col, cats in zip(columns, categories)\n",
    "                for cat in categories[col]\n",
    "            ]\n",
    "            names = df.columns.names + [\"partition\"]\n",
    "        else:\n",
    "            index_tuples = [\n",
    "                (col, cat)\n",
    "                for col, cats in zip(columns, categories)\n",
    "                for cat in categories[col]\n",
    "            ]\n",
    "            names = [df.columns.name, \"partition\"]\n",
    "\n",
    "        new_columns = MultiIndex.from_tuples(index_tuples, names=names)\n",
    "        result = DataFrame(index=df.index, columns=new_columns, dtype=bool)\n",
    "\n",
    "        if isinstance(df.columns, MultiIndex):\n",
    "            for col in new_columns:\n",
    "                result[col] = df[col[:-1]] == col[-1]\n",
    "        else:\n",
    "            for col in new_columns:\n",
    "                result[col] = df[col[0]] == col[-1]\n",
    "\n",
    "        return result\n",
    "\n",
    "    def test_metric(self):\n",
    "        \"\"\"The test metric\"\"\"\n",
    "        return MSE()\n",
    "\n",
    "    @cached_property\n",
    "    def splits(self) -> Mapping:\n",
    "        splits = {}\n",
    "        for key in self.index:\n",
    "            mask = task.split_idx_sparse[key]\n",
    "            ids = task.split_idx_sparse.index[mask]\n",
    "            splits[key] = task.dataset.loc[ids]\n",
    "        return splits\n",
    "\n",
    "    @cached_property\n",
    "    def index(self) -> MultiIndex:\n",
    "        return self.split_idx_sparse.columns\n",
    "\n",
    "    @cached_property\n",
    "    def tensors(self) -> Mapping:\n",
    "        tensors = {}\n",
    "        for _id in self.IDs:\n",
    "            s = self.dataset.loc[_id]\n",
    "            t = torch.tensor(s.index.values, dtype=torch.float32)\n",
    "            x = torch.tensor(s.values, dtype=torch.float32)\n",
    "            tensors[_id] = (t, x)\n",
    "        return tensors\n",
    "\n",
    "    def get_dataloader(\n",
    "        self, key, /, **dataloader_kwargs: Any\n",
    "    ) -> DataLoader:\n",
    "        \"\"\"Return the dataloader for the given key.\"\"\"\n",
    "\n",
    "        fold, partition = key\n",
    "        \n",
    "        dataset = TaskDataset(\n",
    "            {idx:value for idx, val in self.tensors if idx in self.folds[fold][partitions]}\n",
    "            observation_horizon=self.observation_horizon\n",
    "            forecasting_steps=self.forecasting_steps)\n",
    "        \n",
    "        \n",
    "        return DataLoader(dataset, batch_size=32, collate_fn=mycollate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import NamedTuple\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from tsdm.utils.strings import repr_namedtuple, repr_object, repr_type\n",
    "\n",
    "\n",
    "class Inputs(NamedTuple):\n",
    "    r\"\"\"A single sample of the data.\"\"\"\n",
    "    t: Tensor\n",
    "    x: Tensor\n",
    "    t_target: Tensor\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return repr_namedtuple(self, recursive=False)\n",
    "\n",
    "\n",
    "class Sample(NamedTuple):\n",
    "    r\"\"\"A single sample of the data.\"\"\"\n",
    "    key: int\n",
    "    inputs: Inputs\n",
    "    targets: tuple[Tensor, Tensor]\n",
    "    originals: tuple[Tensor, Tensor]\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return repr_namedtuple(self, recursive=False)\n",
    "\n",
    "\n",
    "class Batch(NamedTuple):\n",
    "    r\"\"\"A single sample of the data.\"\"\"\n",
    "    T: Tensor\n",
    "    \"\"\"B×N: the timestamps.\"\"\"\n",
    "    X: Tensor\n",
    "    \"\"\"B×N×D: the observations.\"\"\"\n",
    "    Y: Tensor\n",
    "    \"\"\"B×K×D: the target values.\"\"\"\n",
    "    M: Tensor\n",
    "    \"\"\"B×N: which t correspond to targets.\"\"\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return repr_namedtuple(self, recursive=False)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TaskDataset(torch.utils.data.Dataset):\n",
    "    tensors: dict[int, tuple[Tensor, Tensor]]\n",
    "    observation_horizon: float = 150.0\n",
    "    forecasting_steps: int = 3\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensors)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        t, x = self.tensors[key]\n",
    "        observation_mask = t <= self.observation_horizon\n",
    "        first_target = observation_mask.sum()\n",
    "        target_mask = slice(first_target, first_target + self.forecasting_steps)\n",
    "        return Sample(\n",
    "            key=key,\n",
    "            inputs=Inputs(t[observation_mask], x[observation_mask], t[target_mask]),\n",
    "            targets=x[target_mask],\n",
    "            originals=(t, x),\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}\"\n",
    "\n",
    "\n",
    "def mycollate(batch: list[Sample]) -> Batch:\n",
    "    t_list = []\n",
    "    x_list = []\n",
    "    m_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for sample in batch:\n",
    "        t, x, t_target = sample.inputs\n",
    "        mask = torch.cat(\n",
    "            (torch.zeros_like(t, dtype=bool), torch.ones_like(t_target, dtype=bool))\n",
    "        )\n",
    "        x_padder = torch.full((t_target.shape[0], x.shape[-1]), fill_value=torch.nan)\n",
    "        time = torch.cat((t, t_target))\n",
    "        values = torch.cat((x, x_padder))\n",
    "        idx = torch.argsort(time)\n",
    "        t_list.append(time[idx])\n",
    "        x_list.append(values[idx])\n",
    "        m_list.append(mask[idx])\n",
    "        y_list.append(sample.targets)\n",
    "\n",
    "    T = pad_sequence(t_list, batch_first=True, padding_value=torch.nan).squeeze()\n",
    "    X = pad_sequence(x_list, batch_first=True, padding_value=torch.nan).squeeze()\n",
    "    Y = pad_sequence(y_list, batch_first=True, padding_value=torch.nan).squeeze()\n",
    "    M = pad_sequence(m_list, batch_first=True, padding_value=False).squeeze()\n",
    "\n",
    "    return Batch(T, X, Y, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DS(task.tensors)\n",
    "\n",
    "\n",
    "dloader = DataLoader(dataset, batch_size=32, collate_fn=mycollate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list = []\n",
    "x_list = []\n",
    "m_list = []\n",
    "y_list = []\n",
    "\n",
    "for sample in batch:\n",
    "    t, x, t_target = sample.inputs\n",
    "    mask = torch.cat(\n",
    "        (torch.zeros_like(t, dtype=bool), torch.ones_like(t_target, dtype=bool))\n",
    "    )\n",
    "    x_padder = torch.full((t_target.shape[0], x.shape[-1]), fill_value=torch.nan)\n",
    "    time = torch.cat((t, t_target))\n",
    "    values = torch.cat((x, x_padder))\n",
    "    idx = torch.argsort(time)\n",
    "    t_list.append(time[idx])\n",
    "    x_list.append(values[idx])\n",
    "    m_list.append(mask[idx])\n",
    "    y_list.append(sample.targets)\n",
    "\n",
    "\n",
    "T = pad_sequence(t_list, batch_first=True, padding_value=torch.nan)\n",
    "X = pad_sequence(x_list, batch_first=True, padding_value=torch.nan)\n",
    "M = pad_sequence(m_list, batch_first=True, padding_value=False)\n",
    "Y = pad_sequence(y_list, batch_first=True, padding_value=torch.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.shape for x in batch[2].inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = torch.cat((t_target, t))\n",
    "\n",
    "idx = torch.argsort(zz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch: list[Sample]):\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0].originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[211]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = USHCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 0, \"train\"\n",
    "fold, partition = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample():\n",
    "    obs_mask = t<=150           # first 3 years are observations\n",
    "    val_idx = index[t>150][:3]  # next 3 observations are targets\n",
    "    targets ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = task.splits[0, \"test\"]\n",
    "ids = ts.reset_index()[\"ID\"].unique()\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ids:\n",
    "    print(ts.loc[i].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.split_idx_sparse.index[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = task.folds[0][\"train\"]\n",
    "ts.loc[f0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tsdm.utils import Split\n",
    "\n",
    "IDS = IDX[\"ID\"].unique()\n",
    "\n",
    "num_folds = 5\n",
    "np.random.seed(432)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = ds.reset_index()\n",
    "groups = ts.ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.reset_index().groupby(\"ID\").ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.copy().loc[ds[\"ID\"].isin(folds[0].train)]\n",
    "df.ID = LabelEncoder().fit_transform(df.ID)\n",
    "df = df.sort_values([\"Time\", \"ID\"]).set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gru_ode_bayes\n",
    "from gru_ode_bayes.data_utils import ODE_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ode_ds = ODE_Dataset(panda_df=ds, idx=folds[0].train)\n",
    "ode_ds.df = ode_ds.df.sort_values([\"Time\", \"ID\"])\n",
    "ode_ds.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.index == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ode_ds[0][\"path\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
