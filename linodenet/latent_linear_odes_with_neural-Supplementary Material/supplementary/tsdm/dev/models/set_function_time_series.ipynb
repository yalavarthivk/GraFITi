{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Torch Implementation of Set function for Time Series\n",
    "\n",
    "\n",
    "Base Model: Deep-Set Architecture\n",
    "\n",
    "$$ f(\\mathcal{S})=g\\left(\\frac{1}{|\\mathcal{S}|} \\sum_{s_{j} \\in \\mathcal{S}} h\\left(s_{j}\\right)\\right) $$\n",
    "\n",
    "Modification: Scaled-Dot-Product\n",
    "Paper:\n",
    "\n",
    "$$ K_{j, i}=\\left[f^{\\prime}(\\mathcal{S}), s_{j}\\right]^{T} W_{i}$$\n",
    "\n",
    "where $f'$ is another deep-set model. \n",
    "\n",
    "$$ e_{j, i}=\\frac{K_{j, i} \\cdot Q_{i}}{\\sqrt{d}} \\quad \\text { and } \\quad a_{j, i}=\\frac{\\exp \\left(e_{j, i}\\right)}{\\sum_{j} \\exp \\left(e_{j, i}\\right)}$$\n",
    "\n",
    "> For each head, we multiply\n",
    "the set element embeddings computed via the function h\n",
    "with the attentions derived for the individual instances, i.e.\n",
    "\n",
    "$$ r_{i}=\\sum_{j} a_{j, i} h\\left(s_{j}\\right)$$\n",
    "\n",
    "The final prediction is made by\n",
    "\n",
    "$$  \\hat{y} = g\\Big(\\sum_{s∈S} a(S, s) h(s)\\Big) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Notes\n",
    "\n",
    "- $g$ and $h$ are usually just MLPs, $f'$ is a DeepSet\n",
    "- $m$ is the number of heads\n",
    "- $W$, $Q$ are learnable. $Q$ is initialized with zeros\n",
    "- $W_i$ has shape $(\\dim(f')+\\dim(s), d)$\n",
    "- $Q$ has shape $(m, d)$\n",
    "- $K$ has shape $(|S|, d)$\n",
    "- $E$ has shape $(|S|, m)$\n",
    "- $e_i$ is a vector of size $|S|$\n",
    "- $a_i$ is a vector of size $|S|$\n",
    "- $a(S, s)$ is $(|S|, m)$\n",
    "- $h(s)$ is $(d,)$\n",
    "- $r= [r_1, …, r_m] = \\sum_{s∈S} a(S, s) h(s)$ is of shape $(m,d)$\n",
    "- The authors do not seem to include latent dimension?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Equations\n",
    "\n",
    "Rename: $h = ϕ$, $f' = ρ∘∑∘ψ$\n",
    "\n",
    "$$ a_{j,i} = \\operatorname{softmax}(e_i) = \\sigma(e_i)$$\n",
    "\n",
    "$$ e_{j,i} = \\frac{1}{\\sqrt{d}}K_{j, i}\\cdot Q_{i} = \\frac{1}{\\sqrt{d}}\\left[ψ(\\mathcal{S}), s_{j}\\right]^{T} W_{i}\\cdot Q_{i} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import isqrt, sqrt\n",
    "from typing import Any, Dict, Final, Literal\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torch import Tensor, jit, nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import *\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Tensors for demo purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, Lmax, D = 32, 50, 7\n",
    "m, d = 6, 5\n",
    "\n",
    "batch = [torch.randn(np.random.randint(1, Lmax), D) for _ in range(B)]\n",
    "s = pad_sequence(batch, padding_value=float(\"nan\"), batch_first=True)\n",
    "mask = torch.isnan(s[..., 0])\n",
    "L = int(s.shape[1])\n",
    "s.shape, mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Sequential):\n",
    "    def __init__(self, input_size: int, output_size: int, num_layers: int = 2):\n",
    "\n",
    "        layers = []\n",
    "        for k in range(num_layers):\n",
    "            layer = nn.Linear(input_size, input_size)\n",
    "            nn.init.kaiming_normal_(layer.weight, nonlinearity=\"relu\")\n",
    "            nn.init.kaiming_normal_(layer.bias[None], nonlinearity=\"relu\")\n",
    "            layers.append(layer)\n",
    "            layers.append(nn.ReLU())\n",
    "        else:\n",
    "            layer = nn.Linear(input_size, output_size)\n",
    "            nn.init.kaiming_normal_(layer.weight, nonlinearity=\"relu\")\n",
    "            nn.init.kaiming_normal_(layer.bias[None], nonlinearity=\"relu\")\n",
    "            layers.append(layer)\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "summary(MLP(3, 4, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSet Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSet(nn.Module):\n",
    "    \"\"\"Signature: `[... V, K] -> [... D]`\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        latent_size: Optional[int] = None,\n",
    "        encoder_layers: int = 2,\n",
    "        decoder_layers: int = 2,\n",
    "        # aggregation: Literal[\"min\", \"max\", \"sum\", \"mean\", \"prod\"] = \"sum\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        latent_size = input_size if latent_size is None else latent_size\n",
    "        self.encoder = MLP(input_size, latent_size, encoder_layers)\n",
    "        self.decoder = MLP(latent_size, output_size, decoder_layers)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Signature: [..., <Var>, D] -> [..., F]\n",
    "\n",
    "        Components:\n",
    "\n",
    "          - Encoder: [..., D] -> [..., E]\n",
    "          - Aggregation: [..., V, E] -> [..., E]\n",
    "          - Decoder: [..., E] -> [..., F]\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = torch.nanmean(x, dim=-2)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "summary(DeepSet(3, 4, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = jit.script(DeepSet(7, 4))\n",
    "f(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.permutation(s.shape[1])\n",
    "assert torch.allclose(f(s[..., p, :]), f(s), atol=1e-06), torch.linalg.norm(\n",
    "    f(s[..., p, :]) - f(s)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot Product Attention\n",
    "\n",
    "Keys: $K_{ji} = [f(S), s_j]^T W_i$\n",
    "- $K: |S|×d$. If we want to include batch-size, we need to pad things or operate on lists. \n",
    "    - let's do lists and hope torchscript takes care of it.\n",
    "        - ⟹ But then we need to apply components in \"listified\" manner\n",
    "        - Maybe we can write a decorator that automatically takes care of list inputs?\n",
    "            - Would that work well with torchscript?\n",
    "    - so use padding, but make sure to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.randn(m, d)\n",
    "K = torch.randn(D, m, d)\n",
    "V = torch.randn(D, m, 17)\n",
    "V = torch.einsum(\"...D, DMF -> ...MF \", s, V)\n",
    "print(f\"{Q.shape=}\")\n",
    "K = torch.einsum(\"...f, fmd -> ...md\", s, K)\n",
    "print(f\"{K.shape=}\")\n",
    "QK = torch.einsum(\"...md, md -> ...m\", K, Q) / np.sqrt(d)\n",
    "QK[mask] = float(\"-inf\")\n",
    "print(f\"{QK.shape=}\")\n",
    "σ = nn.functional.softmax(QK, dim=1)\n",
    "print(f\"{σ.shape=}\")\n",
    "print(f\"{V.shape=}\")\n",
    "r = torch.nanmean(σ[..., None] * V, dim=1)\n",
    "print(f\"{r.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_k: int,\n",
    "        dim_v: int,\n",
    "        output_size: int,\n",
    "        num_heads: int = 5,\n",
    "        dim_k_latent: Optional[int] = None,\n",
    "        dim_v_latent: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        dim_q = dim_k\n",
    "\n",
    "        dim_k_latent = max(1, isqrt(dim_k)) if dim_k_latent is None else dim_k_latent\n",
    "        dim_v_latent = dim_v if dim_v_latent is None else dim_v_latent\n",
    "\n",
    "        Wq = torch.zeros((num_heads, dim_k_latent))\n",
    "        Wk = torch.randn((dim_k, num_heads, dim_k_latent)) / sqrt(dim_k)\n",
    "        Wv = torch.randn((dim_v, num_heads, dim_v_latent)) / sqrt(dim_v)\n",
    "        Wo = torch.randn((num_heads, dim_v_latent, output_size)) / sqrt(\n",
    "            num_heads * dim_v_latent\n",
    "        )\n",
    "\n",
    "        self.Wq = nn.Parameter(Wq)\n",
    "        self.Wk = nn.Parameter(Wk)\n",
    "        self.Wv = nn.Parameter(Wv)\n",
    "        self.Wo = nn.Parameter(Wo)\n",
    "        # self.softmax = nn.Softmax(dim=-2)\n",
    "        self.register_buffer(\"scale\", torch.tensor(1 / sqrt(dim_q)))\n",
    "        self.register_buffer(\"attention_weights\", torch.tensor([]))\n",
    "\n",
    "    def forward(self, K: Tensor, V: Tensor, mask: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Q : (h, q)\n",
    "        K : (..., L, d)\n",
    "        V : (..., L, d)\n",
    "        \"\"\"\n",
    "\n",
    "        if mask is None:\n",
    "            mask = torch.isnan(K[..., 0])\n",
    "\n",
    "        Q = self.Wq\n",
    "        K = torch.einsum(\"...d, dhk -> ...hk\", K, self.Wk)\n",
    "        V = torch.einsum(\"...d, dhv -> ...hv\", V, self.Wv)\n",
    "        QK = torch.einsum(\"hd, ...hd -> ...h\", self.Wq, K)\n",
    "        QK[mask] = float(\"-inf\")\n",
    "        w = F.softmax(self.scale * QK, dim=-2)\n",
    "        # w = self.softmax(self.scale * QK)\n",
    "        self.attention_weights = w\n",
    "        QKV = torch.nanmean(w[..., None] * V, dim=-3)  #  ...h,...Lhv -> ...hv\n",
    "        return torch.einsum(\"...hv, hvr -> ...r\", QKV, self.Wo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = jit.script(ScaledDotProductAttention(7, 7, 2))\n",
    "model(s, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.shape, f(s).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(s).repeat(1, L, 1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = torch.tile(f(s).unsqueeze(-2), (L, 1))\n",
    "fs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([fs, s], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsdm.encoders.torch import PositionalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetFuncTS(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        latent_size: Optional[int] = None,\n",
    "        dim_keys: Optional[int] = None,\n",
    "        dim_vals: Optional[int] = None,\n",
    "        dim_time: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        dim_keys = input_size if dim_keys is None else dim_keys\n",
    "        dim_vals = input_size if dim_vals is None else dim_vals\n",
    "        dim_time = 10 if dim_time is None else dim_time\n",
    "        latent_size = input_size if latent_size is None else latent_size\n",
    "        # time_encoder\n",
    "        # feature_encoder -> CNN?\n",
    "        self.time_encoder = PositionalEncoder(dim_time, scale=1.0)\n",
    "        self.key_encoder = DeepSet(input_size + dim_time - 1, dim_keys)\n",
    "        self.value_encoder = MLP(input_size + dim_time - 1, dim_vals)\n",
    "        self.attn = ScaledDotProductAttention(\n",
    "            dim_keys + input_size + dim_time - 1, dim_vals, latent_size\n",
    "        )\n",
    "        self.head = MLP(latent_size, output_size)\n",
    "\n",
    "    def forward(self, s: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        s must be a tensor of the shape L×(2+C), sᵢ = [tᵢ, zᵢ, mᵢ], where\n",
    "        - tᵢ is timestamp\n",
    "        - zᵢ is observed value\n",
    "        - mᵢ is indentifier\n",
    "\n",
    "        C is the number of classes (one-hot encoded identifier)\n",
    "        \"\"\"\n",
    "\n",
    "        t = s[..., 0]\n",
    "        v = s[..., 1:2]\n",
    "        m = s[..., 2:]\n",
    "        time_features = self.time_encoder(t)\n",
    "        s = torch.cat([time_features, v, m], dim=-1)\n",
    "        fs = self.key_encoder(s)\n",
    "        fs = torch.tile(fs.unsqueeze(-2), (s.shape[-2], 1))\n",
    "        K = torch.cat([fs, s], dim=-1)\n",
    "        V = self.value_encoder(s)\n",
    "        mask = torch.isnan(s[..., 0])\n",
    "        z = self.attn(K, V, mask=mask)\n",
    "        y = self.head(z)\n",
    "        return y\n",
    "\n",
    "    @jit.export\n",
    "    def batch_forward(self, s: list[Tensor]) -> Tensor:\n",
    "        return torch.stack([self.forward(x) for x in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = PositionalEncoder(10, 0.9)\n",
    "g.scales\n",
    "g(s[:, :, 0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = jit.script(SetFuncTS(7, 8))\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [torch.randn(16, 7), torch.randn(3, 7), torch.randn(7, 7)]\n",
    "model.batch_forward(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A second heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and some more text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetFuncTS(nn.Module):\n",
    "    def __init__(self, num_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder\n",
    "        self.decoder\n",
    "        self.aggregator\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Signature: `[..., <var>, ]`.\n",
    "\n",
    "        Takes list of triplet-encoded data and applies.\n",
    "        \"\"\"\n",
    "        t = torch.stack(x, dim=-1)\n",
    "        return torch.sum(t, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit.script(SetFuncTS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ">>>>>> input_shapes:                    [(16, 8), (16, 15009, 1), (16, 15009, 1), (16, 15009), (16,)]\n",
    ">>>>>> lengths:                         (16,)\n",
    ">>>>>> max length |S|:                  15009\n",
    ">>>>>> sum lengths ∑|S|:                238416\n",
    ">>>>>> transformed_times:               (16, 15009, 4)\n",
    ">>>>>> transformed_measurements:        (16, 15009, 24)\n",
    ">>>>>> combined_values:                 (16, 15009, 29)\n",
    ">>>>>> demo_encoded:                    (16, 29)\n",
    ">>>>>> combined_with_demo:              (16, 15010, 29)\n",
    ">>>>>> mask:                            (16, 15010)\n",
    ">>>>>> collected_values S:              (238432, 29)\n",
    ">>>>>> encoded ϕ = h(s):                (238432, 256)\n",
    ">>>>>> encoded ψ = f'(S):               (238432, 128)\n",
    ">>>>>> agg ψ:                           (16, 128)\n",
    ">>>>>> agg ρ:                           (16, 128)\n",
    ">>>>>> combined [f(S),s]:               (238432, 157)\n",
    ">>>>>> keys [f(S),s]ᵀW:                 (238432, 4, 1, 128)\n",
    ">>>>>> preattn eᵢⱼ= KQ/√d:              (238432, 4, 1, 128)\n",
    ">>>>>> attentions a(S):                 (4, 238432, 1)\n",
    ">>>>>> weighted_values:                 (4, 238432, 256)\n",
    ">>>>>> weighted_values a(S,s)h(s):      (238432, 1024)\n",
    ">>>>>> aggregated_values ∑a(S,s)h(s):   (16, 1024)\n",
    ">>>>>> output_values g(∑a(S,s)h(s)):    (16, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
