{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb63771-ea67-473c-86e7-efd2003cd400",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'  # always print last expr.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67c2b8a-2f97-4726-adad-7c789c700ea9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "N = 10000\n",
    "\n",
    "t = np.linspace(-np.pi, +np.pi, num=10000)\n",
    "x = np.stack([np.cos(t), np.sin(t)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3fbaf5-3acb-4462-9ad2-dc19fbb8c2a1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Callable, Final, Iterator, Optional, Sized, Union, Generic, TypeVar\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.utils.rnn import (\n",
    "    PackedSequence,\n",
    "    pack_padded_sequence,\n",
    "    pack_sequence,\n",
    "    pad_packed_sequence,\n",
    "    pad_sequence,\n",
    ")\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "__all__: Final[list[str]] = [\n",
    "    \"collate_list\",\n",
    "    \"collate_packed\",\n",
    "    \"collate_padded\",\n",
    "    \"unpad_sequence\",\n",
    "    \"upack_sequence\",\n",
    "]\n",
    "\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "class Sample(Generic[T]):\n",
    "    def __getitem__(self, T) -> Union[T, Callable[[], T]]:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2276e502-37bc-40fd-9879-f5f2d5475fd0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8174fe6-255f-4579-b6c6-3491b0140d95",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(np.array([1, 2, 3]) ** 2) ** (1 / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef4482-710a-458f-9dd5-17dda8abe9e6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tsdm.utils import scaled_norm\n",
    "\n",
    "\n",
    "scaled_norm([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227739dc-676d-44e9-beef-0fcc41ff3a7e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Iterable, List, Type\n",
    "from torch import Tensor\n",
    "from functools import singledispatch\n",
    "\n",
    "# from collections.abc import Iterable\n",
    "import torch\n",
    "\n",
    "\n",
    "@singledispatch\n",
    "def g(x):\n",
    "    pass\n",
    "\n",
    "\n",
    "@g.register\n",
    "def f(x: list[int]) -> list[int]:\n",
    "    return sum(x)\n",
    "\n",
    "\n",
    "g([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6fa25e-508f-4094-96d4-c74481f7c633",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c6970-d4df-449c-a519-fc4646ae9dd4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "isinstance([torch.randn], Iterable[Tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d466b58-9191-4658-9a30-13d2f7f3ec8b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951fb58-d9d8-4f20-980f-9a179b1b0c6d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(\n",
    "    figsize=(16, 10), ncols=2, sharex=True, sharey=True, subplot_kw=dict(box_aspect=1)\n",
    ")\n",
    "ax[0].set_xlim([-1.5, +4.5])\n",
    "ax[0].set_ylim([-1.5, +4.5])\n",
    "ax[1].set_xlim([-1.5, +4.5])\n",
    "ax[1].set_ylim([-1.5, +4.5])\n",
    "for p in (ps := (np.inf, 4, 2, 1, 0.5, 0.25, 0)):\n",
    "    ax[0].plot(*(x / scaled_norm(x, axis=0, p=p)), \"-\")\n",
    "    if p:\n",
    "        ax[1].plot(*(x / np.linalg.norm(x, axis=0, ord=p)), \"-\")\n",
    "    else:\n",
    "        ax[1].plot(\n",
    "            *np.array(\n",
    "                [\n",
    "                    [0, 0],\n",
    "                    [0, 1],\n",
    "                    [0, 0],\n",
    "                    [0, -1],\n",
    "                    [0, 0],\n",
    "                    [-1, 0],\n",
    "                    [0, 0],\n",
    "                    [1, 0],\n",
    "                    [0, 0],\n",
    "                ]\n",
    "            ).T,\n",
    "            \"-\",\n",
    "        )\n",
    "    ax[0].legend(ps)\n",
    "    ax[1].legend(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b883fa6-46b5-4457-8b00-8a64f4f8bb7d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from unittest.mock import ANY\n",
    "\n",
    "\n",
    "def area(p):\n",
    "    assert p.shape == ANY(), 2\n",
    "    x, y = p.T\n",
    "    n = len(x)\n",
    "    shift = (np.arange(n) + 1) % n\n",
    "    return np.sum(y[shift] * x - x[shift] * y) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c510d1-3000-4779-8149-ac31794d8c8d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "r\"\"\"Utility functions.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from collections.abc import Mapping\n",
    "from functools import singledispatch\n",
    "from typing import Any, Final, Iterable, Type, Union, Optional, overload, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from numpy import ndarray\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "from torch import Tensor, nn\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "__all__: Final[list[str]] = [\n",
    "    \"ACTIVATIONS\",\n",
    "    \"deep_dict_update\",\n",
    "    \"deep_kval_update\",\n",
    "    \"relative_error\",\n",
    "    \"scaled_norm\",\n",
    "]\n",
    "\n",
    "\n",
    "ACTIVATIONS: Final[dict[str, Type[nn.Module]]] = {\n",
    "    \"AdaptiveLogSoftmaxWithLoss\": nn.AdaptiveLogSoftmaxWithLoss,\n",
    "    \"ELU\": nn.ELU,\n",
    "    \"Hardshrink\": nn.Hardshrink,\n",
    "    \"Hardsigmoid\": nn.Hardsigmoid,\n",
    "    \"Hardtanh\": nn.Hardtanh,\n",
    "    \"Hardswish\": nn.Hardswish,\n",
    "    \"LeakyReLU\": nn.LeakyReLU,\n",
    "    \"LogSigmoid\": nn.LogSigmoid,\n",
    "    \"LogSoftmax\": nn.LogSoftmax,\n",
    "    \"MultiheadAttention\": nn.MultiheadAttention,\n",
    "    \"PReLU\": nn.PReLU,\n",
    "    \"ReLU\": nn.ReLU,\n",
    "    \"ReLU6\": nn.ReLU6,\n",
    "    \"RReLU\": nn.RReLU,\n",
    "    \"SELU\": nn.SELU,\n",
    "    \"CELU\": nn.CELU,\n",
    "    \"GELU\": nn.GELU,\n",
    "    \"Sigmoid\": nn.Sigmoid,\n",
    "    \"SiLU\": nn.SiLU,\n",
    "    \"Softmax\": nn.Softmax,\n",
    "    \"Softmax2d\": nn.Softmax2d,\n",
    "    \"Softplus\": nn.Softplus,\n",
    "    \"Softshrink\": nn.Softshrink,\n",
    "    \"Softsign\": nn.Softsign,\n",
    "    \"Tanh\": nn.Tanh,\n",
    "    \"Tanhshrink\": nn.Tanhshrink,\n",
    "    \"Threshold\": nn.Threshold,\n",
    "}\n",
    "r\"\"\"Utility dictionary, for use in model creation from Hyperparameter dicts.\"\"\"\n",
    "\n",
    "\n",
    "def _torch_is_float_dtype(x: Tensor) -> bool:\n",
    "    return x.dtype in (\n",
    "        torch.half,\n",
    "        torch.float,\n",
    "        torch.double,\n",
    "        torch.bfloat16,\n",
    "        torch.complex32,\n",
    "        torch.complex64,\n",
    "        torch.complex128,\n",
    "    )\n",
    "\n",
    "\n",
    "def deep_dict_update(d: dict, new_kvals: Mapping) -> dict:\n",
    "    r\"\"\"Update nested dictionary recursively in-place with new dictionary.\n",
    "\n",
    "    Reference: https://stackoverflow.com/a/30655448/9318372\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d: dict\n",
    "    new_kvals: Mapping\n",
    "    \"\"\"\n",
    "    # if not inplace:\n",
    "    #     return deep_dict_update(deepcopy(d), new_kvals, inplace=False)\n",
    "\n",
    "    for key, value in new_kvals.items():\n",
    "        if isinstance(value, Mapping) and value:\n",
    "            d[key] = deep_dict_update(d.get(key, {}), value)\n",
    "        else:\n",
    "            # if value is not None or not safe:\n",
    "            d[key] = new_kvals[key]\n",
    "    return d\n",
    "\n",
    "\n",
    "def deep_kval_update(d: dict, **new_kvals: dict) -> dict:\n",
    "    r\"\"\"Update nested dictionary recursively in-place with key-value pairs.\n",
    "\n",
    "    Reference: https://stackoverflow.com/a/30655448/9318372\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d: dict\n",
    "    new_kvals: dict\n",
    "    \"\"\"\n",
    "    # if not inplace:\n",
    "    #     return deep_dict_update(deepcopy(d), new_kvals, inplace=False)\n",
    "\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, Mapping) and value:\n",
    "            d[key] = deep_kval_update(d.get(key, {}), **new_kvals)\n",
    "        elif key in new_kvals:\n",
    "            # if value is not None or not safe:\n",
    "            d[key] = new_kvals[key]\n",
    "    return d\n",
    "\n",
    "\n",
    "@singledispatch\n",
    "def relative_error(\n",
    "    xhat: Union[ArrayLike, Tensor], x_true: Union[ArrayLike, Tensor]\n",
    ") -> Union[ArrayLike, Tensor]:\n",
    "    r\"\"\"Relative error, works with both :class:`~torch.Tensor` and :class:`~numpy.ndarray`.\n",
    "\n",
    "    .. math::\n",
    "        r(xÌ‚, x) = \\tfrac{|xÌ‚ - x|}{|x|+Ïµ}\n",
    "\n",
    "    The tolerance parameter $Ïµ$ is determined automatically. By default,\n",
    "    $Ïµ=2^{-24}$ for single and $Ïµ=2^{-53}$ for double precision.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xhat: ArrayLike\n",
    "        The estimation\n",
    "    x_true:  ArrayLike\n",
    "        The true value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ArrayLike\n",
    "    \"\"\"\n",
    "    xhat, x_true = np.asanyarray(xhat), np.asanyarray(x_true)\n",
    "    return _numpy_relative_error(xhat, x_true)\n",
    "\n",
    "\n",
    "@relative_error.register\n",
    "def _numpy_relative_error(xhat: ndarray, x_true: ndarray) -> ndarray:\n",
    "    if xhat.dtype in (np.float16, np.int16):\n",
    "        eps = 2**-11\n",
    "    elif xhat.dtype in (np.float32, np.int32):\n",
    "        eps = 2**-24\n",
    "    elif xhat.dtype in (np.float64, np.int64):\n",
    "        eps = 2**-53\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return np.abs(xhat - x_true) / (np.abs(x_true) + eps)\n",
    "\n",
    "\n",
    "@relative_error.register\n",
    "def _torch_relative_error(xhat: Tensor, x_true: Tensor) -> Tensor:\n",
    "    if xhat.dtype in (torch.bfloat16,):\n",
    "        eps = 2**-8\n",
    "    elif xhat.dtype in (torch.float16, torch.int16):\n",
    "        eps = 2**-11\n",
    "    elif xhat.dtype in (torch.float32, torch.int32):\n",
    "        eps = 2**-24\n",
    "    elif xhat.dtype in (torch.float64, torch.int64):\n",
    "        eps = 2**-53\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # eps = eps or _eps\n",
    "    return torch.abs(xhat - x_true) / (torch.abs(x_true) + eps)\n",
    "\n",
    "\n",
    "@singledispatch\n",
    "def scaled_norm(\n",
    "    x: Union[ArrayLike, Tensor],\n",
    "    p: float = 2,\n",
    "    axis: tuple[int] = (),\n",
    "    keepdims: bool = False,\n",
    ") -> Union[NDArray, Tensor]:\n",
    "    r\"\"\"Scaled $â„“^p$-norm, works with both :class:`torch.Tensor` and :class:`numpy.ndarray`.\n",
    "\n",
    "    .. math::\n",
    "        â€–xâ€–_p = (â…Ÿâ‚™ âˆ‘_{i=1}^n |x_i|^p)^{1/p}\n",
    "\n",
    "    This naturally leads to\n",
    "\n",
    "    .. math::\n",
    "       âˆ¥uâŠ•vâˆ¥ = \\frac{\\dim U}{\\dim UâŠ•V} âˆ¥uâˆ¥ + \\frac{\\dim V}{\\dim UâŠ•V} âˆ¥vâˆ¥\n",
    "\n",
    "    This choice is consistent with associativity: $âˆ¥(uâŠ•v)âŠ•wâˆ¥ = âˆ¥uâŠ•(vâŠ•w)âˆ¥$\n",
    "\n",
    "    In particular, given $ð“¤=â¨_{i=1:n} U_i$, then\n",
    "\n",
    "    ..math::\n",
    "        âˆ¥uâˆ¥_p^p = âˆ‘_{i=1:n} \\frac{\\dim U_i}{\\dim ð“¤} âˆ¥u_iâˆ¥_p^p\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: ArrayLike\n",
    "    p: int, default=2\n",
    "    axis: tuple[int], default=None\n",
    "    keepdims: bool, default=False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ArrayLike\n",
    "    \"\"\"\n",
    "    # x = np.asanyarray(x)\n",
    "    # return scaled_norm(x, p=p, axis=axis, keepdims=keepdims)\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "@scaled_norm.register\n",
    "def torch_scaled_norm(\n",
    "    ## type: (Tensor, float, list[int], bool) -> Tensor\n",
    "    x: Tensor,\n",
    "    p: float = 2,\n",
    "    axis: tuple[int, ...] = (),  # TODO: use tuple[int, ...] once supported\n",
    "    keepdims: bool = False,\n",
    ") -> Tensor:\n",
    "\n",
    "    if not _torch_is_float_dtype(x):\n",
    "        x = x.to(dtype=torch.float)\n",
    "    x = torch.abs(x)\n",
    "\n",
    "    if p == 0:\n",
    "        # https://math.stackexchange.com/q/282271/99220\n",
    "        return torch.exp(torch.mean(torch.log(x), dim=axis, keepdim=keepdims))\n",
    "    if p == 1:\n",
    "        return torch.mean(x, dim=axis, keepdim=keepdims)\n",
    "    if p == 2:\n",
    "        return torch.sqrt(torch.mean(x**2, dim=axis, keepdim=keepdims))\n",
    "    if p == float(\"inf\"):\n",
    "        return torch.amax(x, dim=axis, keepdim=keepdims)\n",
    "    # other p\n",
    "    return torch.mean(x**p, dim=axis, keepdim=keepdims) ** (1 / p)\n",
    "\n",
    "\n",
    "@scaled_norm.register\n",
    "def numpy_scaled_norm(\n",
    "    x: ndarray,\n",
    "    p: float = 2,\n",
    "    axis: Union[int, tuple[int, ...]] = None,\n",
    "    keepdims: bool = False,\n",
    ") -> ndarray:\n",
    "    x = np.abs(x)\n",
    "\n",
    "    if p == 0:\n",
    "        # https://math.stackexchange.com/q/282271/99220\n",
    "        return np.exp(np.mean(np.log(x), axis=axis, keepdims=keepdims))\n",
    "    if p == 1:\n",
    "        return np.mean(x, axis=axis, keepdims=keepdims)\n",
    "    if p == 2:\n",
    "        return np.sqrt(np.mean(x**2, axis=axis, keepdims=keepdims))\n",
    "    if p == float(\"inf\"):\n",
    "        return np.max(x, axis=axis, keepdims=keepdims)\n",
    "    # other p\n",
    "    return np.mean(x**p, axis=axis, keepdims=keepdims) ** (1 / p)\n",
    "\n",
    "\n",
    "def flatten_dict(\n",
    "    d: dict[Any, Iterable[Any]], recursive: bool = True\n",
    ") -> list[tuple[Any, ...]]:\n",
    "    r\"\"\"Flatten a dictionary containing iterables to a list of tuples.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d: dict\n",
    "    recursive: bool (default=True)\n",
    "        If true applies flattening strategy recursively on nested dicts, yielding\n",
    "        list[tuple[key1, key2, ...., keyN, value]]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[tuple[Any, ...]]\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for key, iterable in d.items():\n",
    "        for item in iterable:\n",
    "            if isinstance(item, dict) and recursive:\n",
    "                gen: list[tuple[Any, ...]] = flatten_dict(item, recursive=True)\n",
    "                result += [(key,) + tup for tup in gen]\n",
    "            else:\n",
    "                result += [(key, item)]\n",
    "    return result\n",
    "\n",
    "\n",
    "# @torch.jit.script\n",
    "# @scaled_norm.register\n",
    "# def multi_scaled_norm(\n",
    "#     x: list[Tensor],\n",
    "#     p: float = 2,\n",
    "# ) -> Tensor:\n",
    "#     z = torch.stack([scaled_norm(z, p=p) for z in x])\n",
    "#     w = torch.tensor([z.numel() for z in x], device=z.device, dtype=z.dtype)\n",
    "#     return torch.dot(w, z)/torch.sum(w)\n",
    "\n",
    "\n",
    "# How would you call tuples of tensors?\n",
    "# hil-bor hil-tor hil-ber\n",
    "# tup-lor\n",
    "# poly-tor poly-sor\n",
    "# mul-tor mul-sor\n",
    "# n-dor en-dor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79333b25-e563-48a7-80ab-d6a8e546aa15",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(np.random.randn(2, 3, 4), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d34f85-796a-4625-abd4-81c33d737f04",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@singledispatch\n",
    "def _process(x: Tensor) -> Tensor:\n",
    "    return process_torch(x)\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "@_process.register\n",
    "def process_torch(x: Tensor) -> Tensor:\n",
    "    return x\n",
    "\n",
    "\n",
    "@_process.register\n",
    "def process_numpy(x: NDArray) -> NDArray:\n",
    "    return x\n",
    "\n",
    "\n",
    "def process(x: Tensor) -> Tensor:\n",
    "    return _process(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abeb34e-5cf3-4689-b92c-cbe75cf564cc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tsdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f19b56-7934-401b-b0ab-5a761b06054b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def test(x: Tensor) -> Tensor:\n",
    "    return process(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f7e6a8-4e7c-4064-b596-eac6918d3e45",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "process(torch.randn(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eda831-75ed-4fe0-b461-406c5cc0c54f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8756161-56fe-463c-8b0a-5a76046cce3b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scaled_norm([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150dfab8-feef-4479-aa2e-f7ceae54ebd8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
