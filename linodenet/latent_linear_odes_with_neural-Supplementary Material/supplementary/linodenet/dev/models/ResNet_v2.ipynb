{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20191951-beec-414a-b3c6-89dca8277ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'  # always print last expr.\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b7f6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections import OrderedDict\n",
    "from math import sqrt\n",
    "from typing import Any, Optional, cast\n",
    "\n",
    "import torch\n",
    "from linodenet.models.encoders.ft_transformer import (\n",
    "    get_activation_fn,\n",
    "    get_nonglu_activation_fn,\n",
    ")\n",
    "from linodenet.util import (\n",
    "    ReverseDense,\n",
    "    ReZeroCell,\n",
    "    autojit,\n",
    "    deep_dict_update,\n",
    "    initialize_from_config,\n",
    ")\n",
    "from torch import Tensor, jit, nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "__logger__ = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d88691a-c6e8-43a7-af71-6d250d3e186d",
   "metadata": {},
   "source": [
    "## Old ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d695b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@autojit\n",
    "class ResNet_(nn.Module):\n",
    "    \"\"\"Residual Network.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        d_numerical: int,\n",
    "        categories: Optional[list[int]],\n",
    "        d_embedding: int,\n",
    "        d: int,\n",
    "        d_hidden_factor: float,\n",
    "        n_layers: int,\n",
    "        activation: str,\n",
    "        normalization: str,\n",
    "        hidden_dropout: float,\n",
    "        residual_dropout: float,\n",
    "        d_out: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        def make_normalization():\n",
    "            return {\"batchnorm\": nn.BatchNorm1d, \"layernorm\": nn.LayerNorm}[\n",
    "                normalization\n",
    "            ](d)\n",
    "\n",
    "        self.main_activation = get_activation_fn(activation)\n",
    "        self.last_activation = get_nonglu_activation_fn(activation)\n",
    "        self.residual_dropout = residual_dropout\n",
    "        self.hidden_dropout = hidden_dropout\n",
    "\n",
    "        d_in = d_numerical\n",
    "        d_hidden = int(d * d_hidden_factor)\n",
    "\n",
    "        if categories is not None:\n",
    "            d_in += len(categories) * d_embedding\n",
    "            category_offsets = torch.tensor([0] + categories[:-1]).cumsum(0)\n",
    "            self.register_buffer(\"category_offsets\", category_offsets)\n",
    "            self.category_embeddings = nn.Embedding(sum(categories), d_embedding)\n",
    "            nn.init.kaiming_uniform_(self.category_embeddings.weight, a=sqrt(5))\n",
    "            print(f\"{self.category_embeddings.weight.shape=}\")\n",
    "\n",
    "        self.first_layer = nn.Linear(d_in, d)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.ModuleDict(\n",
    "                    {\n",
    "                        \"norm\": make_normalization(),\n",
    "                        \"linear0\": nn.Linear(\n",
    "                            d, d_hidden * (2 if activation.endswith(\"glu\") else 1)\n",
    "                        ),\n",
    "                        \"linear1\": nn.Linear(d_hidden, d),\n",
    "                    }\n",
    "                )\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.last_normalization = make_normalization()\n",
    "        self.head = nn.Linear(d, d_out)\n",
    "\n",
    "    def forward(self, x_num: Tensor, x_cat: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_num: Tensor\n",
    "        x_cat: Optional[Tensor]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "        \"\"\"\n",
    "        tensors = []\n",
    "        if x_num is not None:\n",
    "            tensors.append(x_num)\n",
    "        if x_cat is not None:\n",
    "            assert self.category_embeddings is not None, \"No category embeddings!\"\n",
    "            assert self.category_offsets is not None, \"No category offsets!\"\n",
    "\n",
    "            tensors.append(\n",
    "                self.category_embeddings(\n",
    "                    x_cat + self.category_offsets[None]  # type: ignore[index]\n",
    "                ).view(x_cat.size(0), -1)\n",
    "            )\n",
    "        x = torch.cat(tensors, dim=-1)\n",
    "\n",
    "        x = self.first_layer(x)\n",
    "        for layer in self.layers:\n",
    "            layer = cast(dict[str, nn.Module], layer)\n",
    "            z = x\n",
    "            z = layer[\"norm\"](z)\n",
    "            z = layer[\"linear0\"](z)\n",
    "            z = self.main_activation(z)\n",
    "\n",
    "            if self.hidden_dropout:\n",
    "                z = F.dropout(z, self.hidden_dropout, self.training)\n",
    "\n",
    "            z = layer[\"linear1\"](z)\n",
    "\n",
    "            if self.residual_dropout:\n",
    "                z = F.dropout(z, self.residual_dropout, self.training)\n",
    "            x = x + z\n",
    "        x = self.last_normalization(x)\n",
    "        x = self.last_activation(x)\n",
    "        x = self.head(x)\n",
    "        x = x.squeeze(-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "@autojit\n",
    "class ResNetBlock(nn.Sequential):\n",
    "    \"\"\"Pre-activation ResNet block.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    - | Identity Mappings in Deep Residual Networks\n",
    "      | Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "      | European Conference on Computer Vision 2016\n",
    "      | https://link.springer.com/chapter/10.1007/978-3-319-46493-0_38\n",
    "    \"\"\"\n",
    "\n",
    "    HP = {\n",
    "        \"__name__\": __qualname__,  # type: ignore[name-defined]\n",
    "        \"__module__\": __module__,  # type: ignore[name-defined]\n",
    "        \"input_size\": None,\n",
    "        \"num_subblocks\": 2,\n",
    "        \"subblocks\": [\n",
    "            # {\n",
    "            #     \"__name__\": \"BatchNorm1d\",\n",
    "            #     \"__module__\": \"torch.nn\",\n",
    "            #     \"num_features\": int,\n",
    "            #     \"eps\": 1e-05,\n",
    "            #     \"momentum\": 0.1,\n",
    "            #     \"affine\": True,\n",
    "            #     \"track_running_stats\": True,\n",
    "            # },\n",
    "            ReverseDense.HP,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    def __init__(self, **HP: Any) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.CFG = HP = deep_dict_update(self.HP, HP)\n",
    "\n",
    "        assert HP[\"input_size\"] is not None, \"input_size is required!\"\n",
    "\n",
    "        for layer in HP[\"subblocks\"]:\n",
    "            if layer[\"__name__\"] == \"Linear\":\n",
    "                layer[\"in_features\"] = HP[\"input_size\"]\n",
    "                layer[\"out_features\"] = HP[\"input_size\"]\n",
    "            if layer[\"__name__\"] == \"BatchNorm1d\":\n",
    "                layer[\"num_features\"] = HP[\"input_size\"]\n",
    "            else:\n",
    "                layer[\"input_size\"] = HP[\"input_size\"]\n",
    "                layer[\"output_size\"] = HP[\"input_size\"]\n",
    "\n",
    "        subblocks: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "\n",
    "        for k in range(HP[\"num_subblocks\"]):\n",
    "            key = f\"subblock{k}\"\n",
    "            module = nn.Sequential(\n",
    "                *[initialize_from_config(layer) for layer in HP[\"subblocks\"]]\n",
    "            )\n",
    "            self.add_module(key, module)\n",
    "            subblocks[key] = module\n",
    "\n",
    "        # self.subblocks = nn.Sequential(subblocks)\n",
    "        super().__init__(subblocks)\n",
    "\n",
    "\n",
    "@autojit\n",
    "class ResNet(nn.ModuleList):\n",
    "    \"\"\"A ResNet model.\"\"\"\n",
    "\n",
    "    HP = {\n",
    "        \"__name__\": __qualname__,  # type: ignore[name-defined]\n",
    "        \"__module__\": __module__,  # type: ignore[name-defined]\n",
    "        \"input_size\": None,\n",
    "        \"num_blocks\": 5,\n",
    "        \"blocks\": [\n",
    "            ResNetBlock.HP,\n",
    "            ReZeroCell.HP,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    def __init__(self, **HP: Any) -> None:\n",
    "        super().__init__()\n",
    "        self.CFG = HP = deep_dict_update(self.HP, HP)\n",
    "\n",
    "        assert HP[\"input_size\"] is not None, \"input_size is required!\"\n",
    "\n",
    "        # pass the input_size to the subblocks\n",
    "        for block_cfg in HP[\"blocks\"]:\n",
    "            if \"input_size\" in block_cfg:\n",
    "                block_cfg[\"input_size\"] = HP[\"input_size\"]\n",
    "\n",
    "        blocks: list[nn.Module] = []\n",
    "\n",
    "        for k in range(HP[\"num_blocks\"]):\n",
    "            key = f\"block{k}\"\n",
    "            module = nn.Sequential(\n",
    "                *[initialize_from_config(layer) for layer in HP[\"blocks\"]]\n",
    "            )\n",
    "            self.add_module(key, module)\n",
    "            blocks.append(module)\n",
    "\n",
    "        super().__init__(blocks)\n",
    "\n",
    "    @jit.export\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        r\"\"\"Forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: Tensor\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "        \"\"\"\n",
    "        for block in self:\n",
    "            x = x + block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e438ec6-6599-43f0-856a-33d6556442b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918d7550-e8a2-4204-a742-cc00522bf8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(input_size=2)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696aaa77-1db0-4318-a1a9-58c9357ed6a9",
   "metadata": {},
   "source": [
    "## ResNet V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c5051d-1b30-4299-867b-65226c26a514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar, overload\n",
    "\n",
    "from tsdm.util.decorators import trace\n",
    "\n",
    "Self = TypeVar(\"Self\")  # noqa: Y001\n",
    "\n",
    "\n",
    "@autojit\n",
    "class ResNet(nn.Sequential):\n",
    "    \"\"\"A ResNet model.\"\"\"\n",
    "\n",
    "    HP = {\n",
    "        \"__name__\": __qualname__,  # type: ignore[name-defined]\n",
    "        \"__module__\": __module__,  # type: ignore[name-defined]\n",
    "        \"input_size\": None,\n",
    "        \"num_blocks\": 5,\n",
    "        \"block\": ResNetBlock.HP,\n",
    "    }\n",
    "\n",
    "    def __new__(cls: type[Self], *blocks, **HP) -> Self:\n",
    "        print(f\"__new__   {len(blocks)=} {HP=}\")\n",
    "        assert len(blocks) ^ len(HP), \"Provide either blocks, or hyperparameters!\"\n",
    "\n",
    "        if HP:\n",
    "            return cls.from_hyperparameters(**HP)\n",
    "\n",
    "        return super().__new__(cls)\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        print(f\"__init__, {len(args)=}  {kwargs=}\")\n",
    "        if kwargs:\n",
    "            return\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_hyperparameters(\n",
    "        cls: type[ResNetType],\n",
    "        *,\n",
    "        input_size: int,\n",
    "        num_blocks: int = 5,\n",
    "        block_cfg: dict = ResNetBlock.HP,\n",
    "    ) -> ResNetType:\n",
    "        \"\"\"Create a ResNet model from hyperparameters.\"\"\"\n",
    "\n",
    "        if \"input_size\" in block_cfg:\n",
    "            block_cfg[\"input_size\"] = input_size\n",
    "\n",
    "        blocks: list[nn.Module] = []\n",
    "        for k in range(num_blocks):\n",
    "            module: nn.Module = initialize_from_config(block_cfg)\n",
    "            blocks.append(module)\n",
    "        return cls(*blocks)\n",
    "\n",
    "    @jit.export\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        r\"\"\"Forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: Tensor\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "        \"\"\"\n",
    "        for block in self:\n",
    "            x = x + block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e962bd-405f-4f4e-b264-dcd14a2dcf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet.from_hyperparameters(input_size=2)\n",
    "jit.save(jit.script(model), \"model.pt\")\n",
    "model = jit.load(\"model.pt\")\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb311406-24b9-4eba-82b0-bdeeaa46e916",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(input_size=2)\n",
    "jit.save(jit.script(model), \"model.pt\")\n",
    "model = jit.load(\"model.pt\")\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fcfb13-c701-4b8e-b037-37cc59686d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(nn.Linear(3, 3), nn.Linear(3, 3))\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbde3bb-1d9e-4d2f-90bf-7823974c6dd6",
   "metadata": {},
   "source": [
    "## ReZero v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c22a451-5400-4055-9232-86030352f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "from torch._jit_internal import _copy_to_script_wrapper\n",
    "\n",
    "\n",
    "@autojit\n",
    "class ReZero(ResNet):\n",
    "    r\"\"\"A ReZero model.\"\"\"\n",
    "\n",
    "    def __init__(self, *blocks: nn.Module, **kwargs):\n",
    "        if kwargs:\n",
    "            return\n",
    "        super().__init__(*blocks)\n",
    "        weights = torch.zeros(len(blocks))  # if weights is None else weights\n",
    "        self.register_parameter(\"weights\", nn.Parameter(weights.to(torch.float)))\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    def __getitem__(\n",
    "        self: nn.Sequential, item: Union[int, slice]\n",
    "    ) -> Union[nn.Module, nn.Sequential]:\n",
    "        r\"\"\"Get a sub-model.\"\"\"\n",
    "        modules: list[nn.Module] = list(self._modules.values())\n",
    "        if isinstance(item, slice):\n",
    "            return ReZero(*modules[item], weights=self.weights[item])  # type: ignore[index]\n",
    "        return modules[item]\n",
    "\n",
    "    @jit.export\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.weights)\n",
    "\n",
    "    @jit.export\n",
    "    def length(self) -> int:\n",
    "        return len(self.weights)\n",
    "\n",
    "    @jit.export\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        r\"\"\"Forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: Tensor\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "        \"\"\"\n",
    "        for k, block in enumerate(self):\n",
    "            x = x + self.weights[k] * block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d76be-c756-4f3a-9f45-fcefab4b0854",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ReZero.from_hyperparameters(input_size=2)\n",
    "# print(summary(model))\n",
    "jit.save(jit.script(model), \"model.pt\")\n",
    "model = jit.load(\"model.pt\")\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4142050a-229c-4362-bad2-429072e62b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ReZero(input_size=2)\n",
    "jit.save(jit.script(model), \"model.pt\")\n",
    "model = jit.load(\"model.pt\")\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518d2a2-00f4-43a7-bdbb-a48d70a58dfd",
   "metadata": {},
   "source": [
    "## Series v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e64aeb1-e5ac-419b-94c3-97a68762f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "@autojit\n",
    "class Series(nn.Sequential):\n",
    "    r\"\"\"Pre-activation ResNet block.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    - | Identity Mappings in Deep Residual Networks\n",
    "      | Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "      | European Conference on Computer Vision 2016\n",
    "      | https://link.springer.com/chapter/10.1007/978-3-319-46493-0_38\n",
    "    \"\"\"\n",
    "\n",
    "    HP = {\n",
    "        \"__name__\": __qualname__,  # type: ignore[name-defined]\n",
    "        \"__module__\": __module__,  # type: ignore[name-defined]\n",
    "        \"input_size\": None,\n",
    "        \"layer_cfg\": [\n",
    "            ReverseDense.HP,\n",
    "            ReverseDense.HP,\n",
    "            # {\n",
    "            #     \"__name__\": \"BatchNorm1d\",\n",
    "            #     \"__module__\": \"torch.nn\",\n",
    "            #     \"num_features\": int,\n",
    "            #     \"eps\": 1e-05,\n",
    "            #     \"momentum\": 0.1,\n",
    "            #     \"affine\": True,\n",
    "            #     \"track_running_stats\": True,\n",
    "            # },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    def __new__(cls: type[Self], *modules, **HP) -> Self:\n",
    "        assert not (\n",
    "            len(modules) & len(HP)\n",
    "        ), \"Provide either modules, or hyperparameters!\"\n",
    "\n",
    "        if HP:\n",
    "            return cls.from_hyperparameters(**HP)\n",
    "\n",
    "        return super().__new__(cls)\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        if kwargs:\n",
    "            return\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_hyperparameters(\n",
    "        cls,\n",
    "        *,\n",
    "        input_size: int,\n",
    "        layers_cfg: list[dict] = [ReverseDense.HP, ReverseDense.HP],\n",
    "    ):\n",
    "        \"\"\"Create a Series model from hyperparameters.\"\"\"\n",
    "\n",
    "        for layer in layers_cfg:\n",
    "            if layer[\"__name__\"] == \"Linear\":\n",
    "                layer[\"in_features\"] = input_size\n",
    "                layer[\"out_features\"] = input_size\n",
    "            if layer[\"__name__\"] == \"BatchNorm1d\":\n",
    "                layer[\"num_features\"] = input_size\n",
    "            else:\n",
    "                layer[\"input_size\"] = input_size\n",
    "                layer[\"output_size\"] = input_size\n",
    "\n",
    "        layers: list[nn.Module] = []\n",
    "        for layer_cfg in layers_cfg:\n",
    "            module: nn.Module = initialize_from_config(layer_cfg)\n",
    "            layers.append(module)\n",
    "\n",
    "        return cls(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad46f7-3b55-4dfe-bf9c-1e0dc436d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Series(input_size=5)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe3036f-27a9-456c-a43e-b753cf57858b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Class Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9febd77a-2a29-4bb1-9481-b2b87f7fb3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e14804-89fb-4eba-9727-0f47c5d07ec3",
   "metadata": {},
   "source": [
    "## Repeat v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52753080-77f8-478a-8fbe-d8e9e50fd9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Repeat(nn.Sequential):\n",
    "    \"\"\"Repeat a module multiple times.\"\"\"\n",
    "\n",
    "    HP = {\n",
    "        \"__name__\": __qualname__,  # type: ignore[name-defined]\n",
    "        \"__module__\": __module__,  # type: ignore[name-defined]\n",
    "        \"num_copies\": 2,\n",
    "        \"clone_modules\": True,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self, *args, num_copies: int = 2, clone_layers: bool = True, **kwargs\n",
    "    ) -> None:\n",
    "        if kwargs:\n",
    "            return\n",
    "        super().__init__(*args)\n",
    "\n",
    "    @classmethod\n",
    "    def from_hyperparameters(\n",
    "        cls,\n",
    "        *,\n",
    "        input_size: int,\n",
    "        layers_cfg: list[dict] = [ReverseDense.HP, ReverseDense.HP],\n",
    "        num_copies: int = 2,\n",
    "        clone_layers: bool = True,\n",
    "    ):\n",
    "        \"\"\"Create a Repeat model from hyperparameters.\"\"\"\n",
    "\n",
    "        for layer in layers_cfg:\n",
    "            if layer[\"__name__\"] == \"Linear\":\n",
    "                layer[\"in_features\"] = input_size\n",
    "                layer[\"out_features\"] = input_size\n",
    "            if layer[\"__name__\"] == \"BatchNorm1d\":\n",
    "                layer[\"num_features\"] = input_size\n",
    "            else:\n",
    "                layer[\"input_size\"] = input_size\n",
    "                layer[\"output_size\"] = input_size\n",
    "\n",
    "        layers: list[nn.Module] = []\n",
    "        for layer_cfg in layers_cfg:\n",
    "            module: nn.Module = initialize_from_config(layer_cfg)\n",
    "            layers.append(module)\n",
    "\n",
    "        blocks = []\n",
    "        for k in range(num_copies):\n",
    "            if clone_layers:\n",
    "                block = Series(*[deepcopy(module) for module in layers])\n",
    "            else:\n",
    "                block = Series(*layers)\n",
    "            blocks.append(block)\n",
    "\n",
    "        return cls(*blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ee2265-4020-444d-9397-16e95b99fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Repeat(input_size=5, clone_layers=False)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711d7e49-6bab-45d7-b722-5a4f1c8e8fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34eb782-d933-447a-9d54-44d5ccdac886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
