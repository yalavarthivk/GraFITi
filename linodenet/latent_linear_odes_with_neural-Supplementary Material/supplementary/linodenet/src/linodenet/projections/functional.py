r"""Projections for the Linear ODE Networks.

Notes
-----
Contains projections in functional form.
  - See `~linodenet.projections.modular` for modular implementations.
"""

__all__ = [
    # Functions
    "banded",
    "diagonal",
    "identity",
    "masked",
    "normal",
    "orthogonal",
    "skew_symmetric",
    "symmetric",
]


import torch
from torch import BoolTensor, Tensor, jit


@jit.script
def identity(x: Tensor) -> Tensor:
    r"""Return x as-is.

    .. Signature:: ``(..., n, n) -> (..., n, n)``

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2
    """
    return x


@jit.script
def symmetric(x: Tensor) -> Tensor:
    r"""Return the closest symmetric matrix to X.

    .. Signature:: ``(..., n, n) -> (..., n, n)``

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2 s.t. Y^âŠ¤ = Y

    One can show analytically that Y = Â½(X + X^âŠ¤) is the unique minimizer.
    """
    return (x + x.swapaxes(-1, -2)) / 2


@jit.script
def skew_symmetric(x: Tensor) -> Tensor:
    r"""Return the closest skew-symmetric matrix to X.

    .. Signature:: ``(..., n, n) -> (..., n, n)``

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2 s.t. Y^âŠ¤ = -Y

    One can show analytically that Y = Â½(X - X^âŠ¤) is the unique minimizer.
    """
    return (x - x.swapaxes(-1, -2)) / 2


@jit.script
def normal(x: Tensor) -> Tensor:
    r"""Return the closest normal matrix to X.

    .. Signature:: ``(..., n, n) -> (..., n, n)``

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2 s.t. Y^âŠ¤Y = YY^âŠ¤

    **The Lagrangian:**

    .. math:: â„’(Y, Î›) = Â½âˆ¥X-Yâˆ¥_F^2 + âŸ¨Î›, [Y, Y^âŠ¤]âŸ©

    **First order necessary KKT condition:**

    .. math::
            0 &= âˆ‡â„’(Y, Î›) = (Y-X) + Y(Î› + Î›^âŠ¤) - (Î› + Î›^âŠ¤)Y
        \\âŸº Y &= X + [Y, Î›]

    **Second order sufficient KKT condition:**

    .. math::
             âŸ¨âˆ‡h|SâŸ©=0     &âŸ¹ âŸ¨S|âˆ‡Â²â„’|SâŸ© â‰¥ 0
         \\âŸº âŸ¨[Y, Î›]|SâŸ©=0 &âŸ¹ âŸ¨S|ğ•€âŠ—ğ•€ + Î›âŠ—ğ•€ âˆ’ ğ•€âŠ—Î›|SâŸ© â‰¥ 0
         \\âŸº âŸ¨[Y, Î›]|SâŸ©=0 &âŸ¹ âŸ¨S|SâŸ© + âŸ¨[S, Î›]|SâŸ© â‰¥ 0
    """
    raise NotImplementedError("TODO: implement Fixpoint / Gradient based algorithm.")


@jit.script
def orthogonal(x: Tensor) -> Tensor:
    r"""Return the closest orthogonal matrix to X.

    .. Signature:: ``(..., n, n) -> (..., n, n)``

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2 s.t. Y^ğ–³ Y = ğ•€ = YY^ğ–³

    One can show analytically that $Y = UV^ğ–³$ is the unique minimizer,
    where $X=UÎ£V^ğ–³$ is the SVD of $X$.

    References
    ----------
    - `<https://math.stackexchange.com/q/2215359>`_
    """
    U, _, V = torch.svd(x, some=False, compute_uv=True)
    return torch.einsum("...ij, ...kj -> ...ik", U, V)


@jit.script
def diagonal(x: Tensor) -> Tensor:
    r"""Return the closest diagonal matrix to X.

    .. Signature:: ``(..., n, n) -> (..., n, n)``

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2 s.t. YâŠ™ğ•€ = Y

    One can show analytically that the unique smallest norm minimizer is $Y = ğ•€âŠ™X$.
    """
    eye = torch.eye(x.shape[-1], dtype=torch.bool, device=x.device)
    zero = torch.tensor(0.0, dtype=x.dtype, device=x.device)
    return torch.where(eye, x, zero)


@jit.script
def banded(x: Tensor, u: int = 0, l: int = 0) -> Tensor:
    r"""Return the closest banded matrix to X.

    .. Signature:: ``(..., n, n) -> (..., n, n)``

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2 s.t. YâŠ™B = Y

    One can show analytically that the unique smallest norm minimizer is $Y = BâŠ™X$.
    """
    x = torch.triu(x, diagonal=u)
    x = torch.tril(x, diagonal=l)
    return x


@jit.script
def masked(x: Tensor, m: BoolTensor) -> Tensor:
    r"""Return the closest banded matrix to X.

    .. Signature:: ``(..., n, n) -> (..., n, n)``

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2 s.t. YâŠ™M = Y

    One can show analytically that the unique smallest norm minimizer is $Y = MâŠ™X$.
    """
    zero = torch.tensor(0.0, dtype=x.dtype, device=x.device)
    return torch.where(m, x, zero)
