r"""Different Filter models to be used in conjunction with LinodeNet.

A Filter takes two positional inputs:
    - An input tensor x: the current estimation of the state of the system
    - An input tensor y: the current measurement of the system
    - An optional input tensor mask: a mask to be applied to the input tensor
"""

__all__ = [
    # Constants
    "CELLS",
    # Types
    "Cell",
    # Classes
    "FilterABC",
    "KalmanCell",
    "KalmanFilter",
    "LinearFilter",
    "NonLinearFilter",
    "PseudoKalmanFilter",
    "RecurrentCellFilter",
    "SequentialFilter",
    "SequentialFilterBlock",
]
from abc import abstractmethod
from collections.abc import Iterable
from math import sqrt
from typing import Any, Final, Optional, TypeAlias

import torch
from torch import Tensor, jit, nn

from linodenet.utils import (
    ReverseDense,
    ReZeroCell,
    deep_dict_update,
    deep_keyval_update,
    initialize_from_config,
)

Cell: TypeAlias = nn.Module
r"""Type hint for Cells."""

CELLS: Final[dict[str, type[Cell]]] = {
    "RNNCell": nn.RNNCell,
    "GRUCell": nn.GRUCell,
    "LSTMCell": nn.LSTMCell,
}
r"""Lookup table for cells."""


class FilterABC(nn.Module):
    r"""Base class for all filters.

    All filters should have a signature of the form:

    .. math::  x' = x + Ï•(y-h(x))

    Where $x$ is the current state of the system, $y$ is the current measurement, and
    $x'$ is the new state of the system. $Ï•$ is a function that maps the measurement
    to the state of the system. $h$ is a function that maps the current state of the
    system to the measurement.

    Or multiple blocks of said form. In particular, we are interested in Filters
    satisfying the idempotence property: if $y=h(x)$, then $x'=x$.
    """

    @abstractmethod
    def forward(self, y: Tensor, x: Tensor) -> Tensor:
        r"""Forward pass of the filter.

        Parameters
        ----------
        x: Tensor
            The current estimation of the state of the system.
        y: Tensor
            The current measurement of the system.

        Returns
        -------
        Tensor:
            The updated state of the system.
        """


class PseudoKalmanFilter(FilterABC):
    r"""A Linear, Autoregressive Filter.

    .. math::  xÌ‚' = xÌ‚ - Î±Pâˆâ‚˜áµ€P^{-1}Î â‚˜(xÌ‚ - x)

    - $Î± = 1$ is the "last-value" filter
    - $Î± = 0$ is the "first-value" filter
    - $Î± = Â½$ is the standard Kalman filter, which takes the average between the
      state estimate and the observation.

    One idea: $P = ð•€ + ÎµA$, where $A$ is symmetric. In this case,
    the inverse is approximately given by $ð•€-ÎµA$.

    We define the linearized filter as

    .. math::  xÌ‚' = xÌ‚ - Î±(ð•€ + ÎµA)âˆâ‚˜áµ€(ð•€ - ÎµA)Î â‚˜(xÌ‚ - x)

    Where $Îµ$ is initialized as zero.
    """

    HP = {
        "__name__": __qualname__,  # type: ignore[name-defined]
        "__module__": __module__,  # type: ignore[name-defined]
        "input_size": None,
        "hidden_size": None,
        "alpha": "last-value",
        "alpha_learnable": False,
        "projection": "Symmetric",
    }
    r"""The HyperparameterDict of this class."""

    # CONSTANTS
    input_size: Final[int]
    r"""CONST: The input size (=dim x)."""
    hidden_size: Final[int]
    r"""CONST: The hidden size (=dim y)."""

    # PARAMETERS
    H: Optional[Tensor]
    r"""PARAM: the observation matrix."""
    kernel: Tensor
    r"""PARAM: The kernel matrix."""

    # BUFFERS
    ZERO: Tensor
    r"""BUFFER: A constant value of zero."""

    def __init__(
        self,
        input_size: int,
        alpha: str | float = "last-value",
        alpha_learnable: bool = True,
        projection: str | nn.Module = "symmetric",
        **cfg: Any,
    ):
        super().__init__()
        config = deep_dict_update(self.HP, cfg)

        # CONSTANTS
        self.input_size = input_size
        self.hidden_size = config["hidden_size"]

        # PARAMETERS
        match alpha:
            case "first-value":
                alpha = 0.0
            case "last-value":
                alpha = 1.0
            case "kalman":
                alpha = 0.5
            case str():
                raise ValueError(f"Unknown alpha: {alpha}")

        self.alpha = nn.Parameter(torch.tensor(alpha), requires_grad=alpha_learnable)
        self.epsilon = nn.Parameter(torch.tensor(0.0), requires_grad=True)
        self.weight = nn.Parameter(torch.empty(self.input_size, self.input_size))
        nn.init.kaiming_normal_(self.weight, nonlinearity="linear")

        # BUFFERS
        with torch.no_grad():
            I = torch.eye(self.input_size, dtype=self.weight.dtype)
            kernel = self.epsilon * self.weight
            self.register_buffer("kernel", kernel)
            self.register_buffer("ZERO", torch.zeros(1))
            self.register_buffer("I", I)

    @jit.export
    def forward(self, y: Tensor, x: Tensor) -> Tensor:
        r"""Signature: ``[(..., m), (..., n)] -> (..., n)``."""
        # refresh buffer
        kernel = self.epsilon * self.weight

        # create the mask
        mask = ~torch.isnan(y)  # â†’ [..., m]
        z = torch.where(mask, x - y, self.ZERO)  # â†’ [..., m]
        z = torch.einsum("ij, ...j", self.I - kernel, z)  # â†’ [..., n]
        z = torch.where(mask, z, self.ZERO)
        z = torch.einsum("ij, ...j -> ...i", self.I + kernel, z)
        return x - self.alpha * z


class LinearFilter(FilterABC):
    r"""A Linear Filter.

    .. math::  x' = x - Î±BHáµ€âˆâ‚˜áµ€AÎ â‚˜(Hx - y)

    - $A$ and $B$ are chosen such that

    - $Î± = 1$ is the "last-value" filter
    - $Î± = 0$ is the "first-value" filter
    - $Î± = Â½$ is the standard Kalman filter, which takes the average between the
      state estimate and the observation.

    TODO: Add parametrization options.
    """

    HP = {
        "__name__": __qualname__,  # type: ignore[name-defined]
        "__module__": __module__,  # type: ignore[name-defined]
        "input_size": None,
        "hidden_size": None,
        "alpha": "last-value",
        "alpha_learnable": False,
        "autoregressive": False,
    }
    r"""The HyperparameterDict of this class."""

    # CONSTANTS
    autoregressive: Final[bool]
    r"""CONST: Whether the filter is autoregressive or not."""
    input_size: Final[int]
    r"""CONST: The input size (=dim x)."""
    hidden_size: Final[int]
    r"""CONST: The hidden size (=dim y)."""

    # PARAMETERS
    H: Optional[Tensor]
    r"""PARAM: the observation matrix."""
    kernel: Tensor
    r"""PARAM: The kernel matrix."""

    # BUFFERS
    ZERO: Tensor
    r"""BUFFER: A constant value of zero."""

    def __init__(
        self,
        input_size: int,
        # hidden_size: Optional[int] = None,
        # alpha: str | float = "last-value",
        # alpha_learnable: bool = True,
        # autoregressive: bool = False,
        **cfg: Any,
    ):
        super().__init__()
        config = deep_dict_update(self.HP, cfg)
        hidden_size = (
            input_size if config["hidden_size"] is None else config["hidden_size"]
        )
        alpha = config["alpha"]
        alpha_learnable = config["alpha_learnable"]
        autoregressive = config["autoregressive"]
        assert not autoregressive or input_size == hidden_size

        # CONSTANTS
        self.input_size = n = input_size
        self.hidden_size = m = hidden_size
        self.autoregressive = config["autoregressive"]

        # PARAMETERS
        match alpha:
            case "first-value":
                alpha = 0.0
            case "last-value":
                alpha = 1.0
            case "kalman":
                alpha = 0.5
            case str():
                raise ValueError(f"Unknown alpha: {alpha}")

        # PARAMETERS
        self.alpha = nn.Parameter(torch.tensor(alpha), requires_grad=alpha_learnable)
        self.epsilonA = nn.Parameter(torch.tensor(0.0), requires_grad=True)
        self.epsilonB = nn.Parameter(torch.tensor(0.0), requires_grad=True)
        self.A = nn.Parameter(torch.normal(0, 1 / sqrt(m), size=(m, m)))
        self.B = nn.Parameter(torch.normal(0, 1 / sqrt(n), size=(n, n)))
        self.H = (
            None
            if autoregressive
            else nn.Parameter(torch.normal(0, 1 / sqrt(n), size=(m, n)))
        )
        # TODO: PARAMETRIZATIONS

        # BUFFERS
        self.register_buffer("ZERO", torch.zeros(1))

    @jit.export
    def h(self, x: Tensor) -> Tensor:
        r"""Apply the observation function."""
        if self.autoregressive:
            return x

        # https://pytorch.org/docs/stable/jit_language_reference.html#optional-type-refinement
        H = self.H  # need to assign to local for torchscript....
        assert H is not None, "H must be given in non-autoregressive mode!"
        return torch.einsum("ij, ...j -> ...i", H, x)

    @jit.export
    def ht(self, x: Tensor) -> Tensor:
        r"""Apply the transpose observation function."""
        if self.autoregressive:
            return x

        # https://pytorch.org/docs/stable/jit_language_reference.html#optional-type-refinement
        H = self.H  # need to assign to local for torchscript....
        assert H is not None, "H must be given in non-autoregressive mode!"
        return torch.einsum("ji, ...j -> ...i", H, x)

    @jit.export
    def forward(self, y: Tensor, x: Tensor) -> Tensor:
        r"""Return $x' = x - Î±BHáµ€âˆâ‚˜áµ€AÎ â‚˜(Hx - y)$.

        .. Signature:: ``[(..., m), (..., n)] -> (..., n)``.
        """
        mask = ~torch.isnan(y)  # â†’ [..., m]
        z = self.h(x)
        z = torch.where(mask, z - y, self.ZERO)  # â†’ [..., m]
        z = z + self.epsilonA * torch.einsum("ij, ...j -> ...i", self.A, z)
        z = torch.where(mask, z, self.ZERO)
        z = self.ht(z)
        z = z + self.epsilonB * torch.einsum("ij, ...j -> ...i", self.B, z)
        return x - self.alpha * z


class NonLinearFilter(FilterABC):
    r"""Non-linear Layers stacked on top of linear core."""

    HP = {
        "__name__": __qualname__,  # type: ignore[name-defined]
        "__module__": __module__,  # type: ignore[name-defined]
        "input_size": None,
        "hidden_size": None,
        "autoregressive": False,
        "num_blocks": 2,
        "block": ReverseDense.HP | {"bias": False},
    }
    r"""The HyperparameterDict of this class."""

    # CONSTANTS
    autoregressive: Final[bool]
    r"""CONST: Whether the filter is autoregressive or not."""
    input_size: Final[int]
    r"""CONST: The input size (=dim x)."""
    hidden_size: Final[int]
    r"""CONST: The hidden size (=dim y)."""

    # PARAMETERS
    H: Optional[Tensor]
    r"""PARAM: the observation matrix."""
    kernel: Tensor
    r"""PARAM: The kernel matrix."""

    # BUFFERS
    ZERO: Tensor
    r"""BUFFER: A constant value of zero."""

    def __init__(
        self,
        input_size: int,
        # hidden_size: Optional[int] = None,
        # alpha: str | float = "last-value",
        # alpha_learnable: bool = True,
        # autoregressive: bool = False,
        **cfg: Any,
    ):
        super().__init__()
        config = deep_dict_update(self.HP, cfg)
        hidden_size = (
            input_size if config["hidden_size"] is None else config["hidden_size"]
        )
        autoregressive = config["autoregressive"]
        config["block"]["input_size"] = input_size
        config["block"]["output_size"] = input_size
        assert not autoregressive or input_size == hidden_size

        # CONSTANTS
        self.input_size = n = input_size
        self.hidden_size = m = hidden_size
        self.autoregressive = config["autoregressive"]

        # MODULES
        blocks: list[nn.Module] = []
        for _ in range(config["num_blocks"]):
            module = initialize_from_config(config["block"])
            if hasattr(module, "bias"):
                assert module.bias is None, "Avoid bias term!"
            blocks.append(module)

        self.layers = nn.Sequential(*blocks)

        # PARAMETERS
        self.epsilon = nn.Parameter(torch.tensor(0.0), requires_grad=True)
        # self.epsilonA = nn.Parameter(torch.tensor(0.0), requires_grad=True)
        # self.epsilonB = nn.Parameter(torch.tensor(0.0), requires_grad=True)
        self.A = nn.Parameter(torch.normal(0, 1 / sqrt(m), size=(m, m)))
        self.B = nn.Parameter(torch.normal(0, 1 / sqrt(n), size=(n, n)))
        self.H = (
            None
            if autoregressive
            else nn.Parameter(torch.normal(0, 1 / sqrt(n), size=(m, n)))
        )
        # TODO: PARAMETRIZATIONS

        # BUFFERS
        self.register_buffer("ZERO", torch.zeros(1))

    @jit.export
    def h(self, x: Tensor) -> Tensor:
        r"""Apply the observation function."""
        if self.autoregressive:
            return x

        # https://pytorch.org/docs/stable/jit_language_reference.html#optional-type-refinement
        H = self.H  # need to assign to local for torchscript....
        assert H is not None, "H must be given in non-autoregressive mode!"
        return torch.einsum("ij, ...j -> ...i", H, x)

    @jit.export
    def ht(self, x: Tensor) -> Tensor:
        r"""Apply the transpose observation function."""
        if self.autoregressive:
            return x

        # https://pytorch.org/docs/stable/jit_language_reference.html#optional-type-refinement
        H = self.H  # need to assign to local for torchscript....
        assert H is not None, "H must be given in non-autoregressive mode!"
        return torch.einsum("ji, ...j -> ...i", H, x)

    @jit.export
    def forward(self, y: Tensor, x: Tensor) -> Tensor:
        r"""Return $x' = x - Î±BHáµ€âˆâ‚˜áµ€AÎ â‚˜(Hx - y)$.

        .. Signature:: ``[(..., m), (..., n)] -> (..., n)``.
        """
        mask = ~torch.isnan(y)  # (..., m)
        z = self.h(x)  # (..., m)
        z = torch.where(mask, z - y, self.ZERO)  # (..., m)
        z = torch.einsum("ij, ...j -> ...i", self.A, z)
        z = torch.where(mask, z, self.ZERO)  # (..., m)
        z = self.ht(z)  # (..., n)
        z = torch.einsum("ij, ...j -> ...i", self.B, z)
        return x - self.epsilon * self.layers(z)


class KalmanFilter(FilterABC):
    r"""Classical Kalman Filter.

    .. math::
        xÌ‚â‚œâ‚Šâ‚ &= xÌ‚â‚œ + Pâ‚œ Hâ‚œáµ€(Hâ‚œ Pâ‚œ   Hâ‚œáµ€ + Râ‚œ)â»Â¹ (yâ‚œ - Hâ‚œ xÌ‚â‚œ) \\
        Pâ‚œâ‚Šâ‚ &= Pâ‚œ - Pâ‚œ Hâ‚œáµ€(Hâ‚œ Pâ‚œâ»Â¹ Hâ‚œáµ€ + Râ‚œ)â»Â¹ Hâ‚œ Pâ‚œâ»Â¹

    In the case of missing data:

    Substitute $yâ‚œâ† Sâ‚œâ‹…yâ‚œ$, $Hâ‚œ â† Sâ‚œâ‹…Hâ‚œ$ and $Râ‚œ â† Sâ‚œâ‹…Râ‚œâ‹…Sâ‚œáµ€$ where $Sâ‚œ$
    is the $mâ‚œÃ—m$ projection matrix of the missing values. In this case:

    .. math::
        xÌ‚' &= xÌ‚ + Pâ‹…Háµ€â‹…Sáµ€(SHPHáµ€Sáµ€ + SRSáµ€)â»Â¹ (Sy - SHxÌ‚) \\
           &= xÌ‚ + Pâ‹…Háµ€â‹…Sáµ€(S (HPHáµ€ + R) Sáµ€)â»Â¹ S(y - HxÌ‚) \\
           &= xÌ‚ + Pâ‹…Háµ€â‹…(SâºS)áµ€ (HPHáµ€ + R)â»Â¹ (SâºS) (y - HxÌ‚) \\
           &= xÌ‚ + Pâ‹…Háµ€â‹…âˆâ‚˜áµ€ (HPHáµ€ + R)â»Â¹ âˆâ‚˜ (y - HxÌ‚) \\
        P' &= P - Pâ‹…Háµ€â‹…Sáµ€(S H Pâ»Â¹ Háµ€ Sáµ€ + SRSáµ€)â»Â¹ SH Pâ»Â¹ \\
           &= P - Pâ‹…Háµ€â‹…(SâºS)áµ€ (H Pâ»Â¹ Háµ€ + R)â»Â¹ (SâºS) H Pâ»Â¹ \\
           &= P - Pâ‹…Háµ€â‹…âˆâ‚˜áµ€ (H Pâ»Â¹ Háµ€ + R)â»Â¹ âˆâ‚˜ H Pâ»Â¹


    .. note::
        The Kalman filter is a linear filter. The non-linear version is also possible,
        the so called Extended Kalman-Filter. Here, the non-linearity is linearized at
        the time of update.

        ..math ::
            xÌ‚' &= xÌ‚ + Pâ‹…Háµ€(HPHáµ€ + R)â»Â¹ (y - h(xÌ‚)) \\
            P' &= P -  Pâ‹…Háµ€(HPHáµ€ + R)â»Â¹ H P

        where $H = \frac{âˆ‚h}{âˆ‚x}|_{xÌ‚}$. Note that the EKF is generally not an optimal
        filter.
    """

    # CONSTANTS
    input_size: Final[int]
    r"""CONST: The input size."""
    hidden_size: Final[int]
    r"""CONST: The hidden size."""

    # PARAMETERS
    H: Tensor
    r"""PARAM: The observation matrix."""
    R: Tensor
    r"""PARAM: The observation noise covariance matrix."""

    # BUFFERS
    ZERO: Tensor
    r"""BUFFER: A constant value of zero."""

    def __init__(self, /, input_size: int, hidden_size: int):
        super().__init__()

        # CONSTANTS
        self.input_size = input_size
        self.hidden_size = hidden_size

        # BUFFERS
        self.register_buffer("ZERO", torch.zeros(1))

        # PARAMETERS
        self.H = nn.Parameter(torch.empty(input_size, hidden_size))
        self.R = nn.Parameter(torch.empty(hidden_size, hidden_size))
        nn.init.kaiming_normal_(self.H, nonlinearity="linear")
        nn.init.kaiming_normal_(self.R, nonlinearity="linear")

    @jit.export
    def forward(self, y: Tensor, x: Tensor, *, P: Optional[Tensor] = None) -> Tensor:
        r"""Forward pass of the filter."""
        P = torch.eye(x.shape[-1]) if P is None else P
        # create the mask
        mask = ~torch.isnan(y)
        H = self.H
        R = self.R
        r = torch.einsum("ij, ...j -> ...i", H, x) - y
        r = torch.where(mask, r, self.ZERO)
        z = torch.linalg.solve(H @ P @ H.t() + R, r)
        z = torch.where(mask, z, self.ZERO)
        return x - torch.einsum("ij, jk, ..k -> ...i", P, H.t(), z)


class KalmanCell(FilterABC):
    r"""A Kalman-Filter inspired non-linear Filter.

    We assume that $y = h(x)$ and $y = Hâ‹…x$ in the linear case. We adapt  the formula
    provided by the regular Kalman Filter and replace the matrices with learnable
    parameters $A$ and $B$ and insert an neural network block $Ïˆ$, typically a
    non-linear activation function followed by a linear layer $Ïˆ(z)=WÏ•(z)$.

    .. math::
        xÌ‚' &= xÌ‚ + Pâ‹…Háµ€ âˆâ‚˜áµ€ (HPHáµ€ + R)â»Â¹ âˆâ‚˜ (y - HxÌ‚)    \\
           &â‡ xÌ‚ + Bâ‹…Háµ€ âˆâ‚˜áµ€Aâˆâ‚˜ (y - HxÌ‚)                 \\
           &â‡ xÌ‚ + Ïˆ(B Háµ€ âˆâ‚˜áµ€A âˆâ‚˜ (y - HxÌ‚))

    Here $yâ‚œ$ is the observation vector. and $xÌ‚$ is the state vector.


    .. math::
        xÌ‚' &= xÌ‚ - Pâ‹…Háµ€ âˆâ‚˜áµ€ (HPHáµ€ + R)â»Â¹ âˆâ‚˜ (HxÌ‚ - y)    \\
           &â‡ xÌ‚ - Bâ‹…Háµ€ âˆâ‚˜áµ€Aâˆâ‚˜ (HxÌ‚ - y)                 \\
           &â‡ xÌ‚ - Ïˆ(B Háµ€ âˆâ‚˜áµ€A âˆâ‚˜ (HxÌ‚ - y))

    Note that in the autoregressive case, $H=ð•€$ and $P=R$. Thus

    .. math::
        xÌ‚' &= xÌ‚ - Pâˆâ‚˜áµ€(2P)â»Â¹Î â‚˜(xÌ‚ - x)        \\
           &= xÌ‚ - Â½ Pâˆâ‚˜áµ€P^{-1}Î â‚˜(xÌ‚ - y)      \\

    We consider a few cases:

    .. math::  xÌ‚' = xÌ‚ - Î±(xÌ‚ - x)

    - $Î± = 1$ is the "last-value" filter
    - $Î± = 0$ is the "first-value" filter
    - $Î± = Â½$ is the standard Kalman filter, which takes the average between the
      state estimate and the observation.

    So in this case, the filter precisely always chooses the average between the prediction and the measurement.

    The reason for a another linear transform after $Ï•$ is to stabilize the distribution.
    Also, when $Ï•=ð–±ð–¾ð–«ð–´$, it is necessary to allow negative updates.

    Note that in the autoregressive case, i.e. $H=ð•€$, the equation can be simplified
    towards $xÌ‚' â‡ xÌ‚ + Ïˆ( B âˆâ‚˜áµ€ A âˆâ‚˜ (y - HxÌ‚) )$.

    References
    ----------
    - | Kalman filter with outliers and missing observations
      | T. Cipra, R. Romera
      | https://link.springer.com/article/10.1007/BF02564705
    """

    HP = {
        "__name__": __qualname__,  # type: ignore[name-defined]
        "__module__": __module__,  # type: ignore[name-defined]
        "input_size": None,
        "hidden_size": None,
        "autoregressive": False,
    }
    r"""The HyperparameterDict of this class."""

    # CONSTANTS
    autoregressive: Final[bool]
    r"""CONST: Whether the filter is autoregressive or not."""
    input_size: Final[int]
    r"""CONST: The input size (=dim x)."""
    hidden_size: Final[int]
    r"""CONST: The hidden size (=dim y)."""

    # PARAMETERS
    H: Optional[Tensor]
    r"""PARAM: the observation matrix."""
    kernel: Tensor
    r"""PARAM: The kernel matrix."""

    # BUFFERS
    ZERO: Tensor
    r"""BUFFER: A constant value of zero."""

    def __init__(self, /, **cfg: Any):
        super().__init__()
        config = deep_dict_update(self.HP, cfg)

        # CONSTANTS
        self.autoregressive = config["autoregressive"]
        self.input_size = input_size = config["input_size"]

        if self.autoregressive:
            hidden_size = config["input_size"]
        else:
            hidden_size = config["hidden_size"]

        self.hidden_size = hidden_size

        # BUFFERS
        self.register_buffer("ZERO", torch.zeros(1))

        # PARAMETERS
        self.A = nn.Parameter(torch.empty(hidden_size, hidden_size))
        self.B = nn.Parameter(torch.empty(input_size, input_size))
        nn.init.kaiming_normal_(self.A, nonlinearity="linear")
        nn.init.kaiming_normal_(self.B, nonlinearity="linear")

        if self.autoregressive:
            assert (
                hidden_size == input_size
            ), "Autoregressive filter requires x_dim == y_dim"
            self.H = None
        else:
            self.H = nn.Parameter(torch.empty(hidden_size, input_size))
            nn.init.kaiming_normal_(self.H, nonlinearity="linear")

    @jit.export
    def h(self, x: Tensor) -> Tensor:
        r"""Apply the observation function."""
        if self.autoregressive:
            return x

        # https://pytorch.org/docs/stable/jit_language_reference.html#optional-type-refinement
        H = self.H  # need to assign to local for torchscript....
        assert H is not None, "H must be given in non-autoregressive mode!"
        return torch.einsum("ij, ...j -> ...i", H, x)

    @jit.export
    def ht(self, x: Tensor) -> Tensor:
        r"""Apply the transpose observation function."""
        if self.autoregressive:
            return x

        # https://pytorch.org/docs/stable/jit_language_reference.html#optional-type-refinement
        H = self.H  # need to assign to local for torchscript....
        assert H is not None, "H must be given in non-autoregressive mode!"
        return torch.einsum("ji, ...j -> ...i", H, x)

    @jit.export
    def forward(self, y: Tensor, x: Tensor) -> Tensor:
        r"""Return $BÎ AÎ (x - y)$.

        .. Signature:: ``[(..., m), (..., n)] -> (..., n)``.
        """
        mask = ~torch.isnan(y)  # â†’ [..., m]
        r = torch.where(mask, self.h(x) - y, self.ZERO)  # â†’ [..., m]
        z = torch.where(mask, torch.einsum("ij, ...j -> ...i", self.A, r), self.ZERO)
        return torch.einsum("ij, ...j -> ...i", self.B, self.ht(z))


# class SequentialFilterBlock(FilterABC, nn.ModuleList):
#     r"""Multiple Filters applied sequentially."""
#
#     HP = {
#         "__name__": __qualname__,  # type: ignore[name-defined]
#         "__module__": __module__,  # type: ignore[name-defined]
#         "input_size": None,
#         "filter": KalmanCell.HP | {"autoregressive": True},
#         "layers": [ReverseDense.HP | {"bias": False}, ReZeroCell.HP],
#     }
#     r"""The HyperparameterDict of this class."""
#
#     input_size: Final[int]
#
#     def __init__(self, *args: Any, **HP: Any) -> None:
#         super().__init__()
#         self.CFG = HP = deep_dict_update(self.HP, HP)
#
#         self.input_size = input_size = HP["input_size"]
#         HP["filter"]["input_size"] = input_size
#
#         layers: list[nn.Module] = []
#
#         for layer in HP["layers"]:
#             if "input_size" in layer:
#                 layer["input_size"] = input_size
#             if "output_size" in layer:
#                 layer["output_size"] = input_size
#             module = initialize_from_config(layer)
#             layers.append(module)
#
#         layers = list(args) + layers
#         self.filter: nn.Module = initialize_from_config(HP["filter"])
#         self.layers: Iterable[nn.Module] = nn.Sequential(*layers)
#
#     @jit.export
#     def forward(self, y: Tensor, x: Tensor) -> Tensor:
#         r"""Signature: ``[(..., m), (..., n)] -> (..., n)``."""
#         z = self.filter(y, x)
#         for module in self.layers:
#             z = module(z)
#         return x + z


class SequentialFilterBlock(FilterABC):
    r"""Non-linear Layers stacked on top of linear core."""

    HP = {
        "__name__": __qualname__,  # type: ignore[name-defined]
        "__module__": __module__,  # type: ignore[name-defined]
        "input_size": None,
        "autoregressive": False,
        "filter": KalmanCell.HP,
        "layers": [ReverseDense.HP | {"bias": False}, ReZeroCell.HP],
    }
    r"""The HyperparameterDict of this class."""

    input_size: Final[int]

    def __init__(
        self, modules: Optional[Iterable[nn.Module]] = None, **cfg: Any
    ) -> None:
        super().__init__()
        config = deep_dict_update(self.HP, cfg)
        config["filter"]["autoregressive"] = config["autoregressive"]

        self.input_size = input_size = config["input_size"]
        config["filter"]["input_size"] = input_size

        self.nonlinear: nn.Module = initialize_from_config(config["filter"])

        # self.add_module("nonlinear", nonlinear)

        layers: list[nn.Module] = [] if modules is None else list(modules)
        for layer in config["layers"]:
            if "input_size" in layer:
                layer["input_size"] = input_size
            if "output_size" in layer:
                layer["output_size"] = input_size
            module = initialize_from_config(layer)
            layers.append(module)

        # super().__init__(layers)
        self.layers = nn.Sequential(*layers)

    @jit.export
    def forward(self, y: Tensor, x: Tensor) -> Tensor:
        r"""Signature: ``[(..., m), (..., n)] -> (..., n)``."""
        z = self.nonlinear(y, x)
        for module in self.layers:
            z = module(z)
        return x - z


class SequentialFilter(FilterABC, nn.Sequential):
    r"""Multiple Filters applied sequentially."""

    HP = {
        "__name__": __qualname__,  # type: ignore[name-defined]
        "__module__": __module__,  # type: ignore[name-defined]
        "input_size": None,
        "hidden_size": None,
        "autoregressive": False,
        "layers": [LinearFilter.HP, NonLinearFilter.HP, NonLinearFilter.HP],
    }
    r"""The HyperparameterDict of this class."""

    def __init__(self, *modules: nn.Module, **cfg: Any) -> None:
        config = deep_dict_update(self.HP, cfg)

        layers: list[nn.Module] = [] if modules is None else list(modules)

        for layer in config["layers"]:
            if isinstance(layer, nn.Module):
                module = layer
            else:
                layer["autoregressive"] = config["autoregressive"]
                layer["input_size"] = config["input_size"]
                layer["hidden_size"] = config["hidden_size"]
                module = initialize_from_config(layer)
            layers.append(module)

        nn.Sequential.__init__(self, *layers)

    @jit.export
    def forward(self, y: Tensor, x: Tensor) -> Tensor:
        r"""Signature: ``[(..., m), (..., n)] -> (..., n)``."""
        for module in self:
            x = module(y, x)
        return x


class RecurrentCellFilter(FilterABC):
    r"""Any Recurrent Cell allowed."""

    HP = {
        "__name__": __qualname__,  # type: ignore[name-defined]
        "__module__": __module__,  # type: ignore[name-defined]
        "concat": True,
        "input_size": None,
        "hidden_size": None,
        "autoregressive": True,
        "Cell": {
            "__name__": "GRUCell",
            "__module__": "torch.nn",
            "input_size": None,
            "hidden_size": None,
            "bias": True,
            "device": None,
            "dtype": None,
        },
    }
    r"""The HyperparameterDict of this class."""

    # CONSTANTS
    concat_mask: Final[bool]
    r"""CONST: Whether to concatenate the mask to the inputs."""
    input_size: Final[int]
    r"""CONST: The input size."""
    hidden_size: Final[int]
    r"""CONST: The hidden size."""
    autoregressive: Final[bool]
    r"""CONST: Whether the filter is autoregressive or not."""

    # PARAMETERS
    H: Tensor
    r"""PARAM: the observation matrix."""

    def __init__(self, /, input_size: int, hidden_size: int, **cfg: Any):
        super().__init__()
        config = deep_dict_update(self.HP, cfg)

        # CONSTANTS
        self.concat_mask = config["concat"]
        self.input_size = input_size * (1 + self.concat_mask)
        self.hidden_size = hidden_size
        self.autoregressive = config["autoregressive"]

        if self.autoregressive:
            assert (
                hidden_size == input_size
            ), "Autoregressive filter requires x_dim == y_dim"
            self.H = torch.eye(input_size)
        else:
            self.H = nn.Parameter(torch.empty(input_size, hidden_size))
            nn.init.kaiming_normal_(self.H, nonlinearity="linear")

        deep_keyval_update(
            config, input_size=self.input_size, hidden_size=self.hidden_size
        )

        # MODULES
        self.cell = initialize_from_config(config["Cell"])

    @jit.export
    def h(self, x: Tensor) -> Tensor:
        r"""Apply the observation function."""
        if self.autoregressive:
            return x
        return torch.einsum("ij, ...j -> ...i", self.H, x)

    @jit.export
    def forward(self, y: Tensor, x: Tensor) -> Tensor:
        r"""Signature: ``[(..., m), (..., n)] -> (..., n)``."""
        mask = torch.isnan(y)

        # impute missing value in observation with state estimate
        if self.autoregressive:
            y = torch.where(mask, x, y)
        else:
            # TODO: something smarter in non-autoregressive case
            y = torch.where(mask, self.h(x), y)

        if self.concat_mask:
            y = torch.cat([y, mask], dim=-1)

        # Flatten for RNN-Cell
        y = y.view(-1, y.shape[-1])
        x = x.view(-1, x.shape[-1])

        # Apply filter
        result = self.cell(y, x)

        # De-Flatten return value
        return result.view(mask.shape)
